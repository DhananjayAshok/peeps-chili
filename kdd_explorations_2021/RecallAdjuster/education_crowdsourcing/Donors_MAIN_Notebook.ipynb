{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemank/.pyenv/versions/3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:15: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display \n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline\n",
    "\n",
    "def connect(poolclass=sqlalchemy.pool.QueuePool):\n",
    "    with open(os.path.join(os.path.join('../../..','config'), 'donors_db_profile.yaml')) as fd:\n",
    "        config = yaml.load(fd)\n",
    "        dburl = sqlalchemy.engine.url.URL(\n",
    "            \"postgres\",\n",
    "            host=config[\"host\"],\n",
    "            username=config[\"user\"],\n",
    "            database=config[\"db\"],\n",
    "            password=config[\"pass\"],\n",
    "            port=config[\"port\"],\n",
    "        )\n",
    "        return sqlalchemy.create_engine(dburl, poolclass=poolclass)\n",
    "\n",
    "    \n",
    "conn = connect()\n",
    "\n",
    "import SAVE_RecallAdjuster as ra\n",
    "from importlib import reload\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2010-05-01', '2010-05-01'), ('2010-05-01', '2010-07-01'), ('2010-07-01', '2010-07-01'), ('2010-07-01', '2010-09-01'), ('2010-09-01', '2010-09-01'), ('2010-09-01', '2010-11-01'), ('2010-11-01', '2010-11-01'), ('2010-11-01', '2011-01-01'), ('2011-01-01', '2011-01-01'), ('2011-01-01', '2011-03-01'), ('2011-03-01', '2011-03-01'), ('2011-03-01', '2011-05-01'), ('2011-05-01', '2011-05-01'), ('2011-05-01', '2011-07-01'), ('2011-07-01', '2011-07-01'), ('2011-07-01', '2011-09-01'), ('2011-09-01', '2011-09-01'), ('2011-09-01', '2011-11-01'), ('2011-11-01', '2011-11-01'), ('2011-11-01', '2012-01-01'), ('2012-01-01', '2012-01-01'), ('2012-01-01', '2012-03-01'), ('2012-03-01', '2012-03-01'), ('2012-03-01', '2012-05-01'), ('2012-05-01', '2012-05-01'), ('2012-05-01', '2012-07-01'), ('2012-07-01', '2012-07-01'), ('2012-07-01', '2012-09-01'), ('2012-09-01', '2012-09-01'), ('2012-09-01', '2012-11-01'), ('2012-11-01', '2012-11-01'), ('2012-11-01', '2013-01-01'), ('2013-01-01', '2013-01-01'), ('2013-01-01', '2013-03-01')]\n"
     ]
    }
   ],
   "source": [
    "base = datetime.datetime.strptime('2013-03-01', '%Y-%m-%d')   #Corresponding to latest train_end_time\\n\",\n",
    "date_pairs = []\n",
    "for x in range(16, -1, -1):\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=2*x) - relativedelta(months=2)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=2*x) - relativedelta(months=2)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=2*x) - relativedelta(months=2)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=2*x)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "print(date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SAVE_RecallAdjuster as sra\n",
    "from jinja2 import Template\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Save Recall Adjuster for all Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_under_1_orig_orig',\n",
    "        experiment_hashes='a4bf9cbc91636017f7261a4008fb4142',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_under_orig_50_50',\n",
    "        experiment_hashes='78003ff2591febd2ddd5b66c2a40a376',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_under_orig_50_orig',\n",
    "        experiment_hashes='f39fdc1f199f7197c33dd97729c30587',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_under_orig_snop_orig',\n",
    "        experiment_hashes='072c8001ed06f6c40725b8ec7f4766c2',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_under_1_50_50',\n",
    "        experiment_hashes='0de125d2f18da08a0cc47965fbf26c34',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_under_1_snop_orig',\n",
    "        experiment_hashes='585f893d800daa42f49b40f7bc8fb7c8',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_over_1_orig_orig',\n",
    "        experiment_hashes='3df029a03f24c7ee757e059ee7408111',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_over_orig_50_50',\n",
    "        experiment_hashes='00641e9c873e781426b851cc1c353d91',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_over_orig_50_orig',\n",
    "        experiment_hashes='20f0b464acc172e7aa64927bd52666c8',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_over_orig_snop_orig',\n",
    "        experiment_hashes='e0811375bd4df985dee9d02dbecaf98b',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_over_1_50_50',\n",
    "        experiment_hashes='fd5d66b37e71b1144ffd050bceb9d876',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_orig = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_over_1_snop_orig',\n",
    "        experiment_hashes='0d063594033baeccf1889e6be7708137',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoupled Recall Adjuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.dispose()\n",
    "conn = connect()\n",
    "reload(sra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRA_decoupled = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='rg_staff',\n",
    "        schema='donors_bias_decoupled',\n",
    "        experiment_hashes='10ec0614881ab8f1d97754c863e39fd3',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[500],\n",
    "        #entity_demos='joco',\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        decoupled_experiments=[\n",
    "            ('4fcb39da1328adf4e162f1e01f226745', 'highest'), \n",
    "            ('36623fcacc16535159a0f9e2bf11c9e2', 'not_highest')],\n",
    "        decoupled_entity_demos='hemank_bias_original.entity_demos3',\n",
    "        dataset='donors',\n",
    "        model_group_ids = [906, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918,\n",
    "                          943, 944, 947, 948, 951, 952, 955, 956, 959, 960, 963, 964,\n",
    "                          994, 995, 1153, 1160, 1161, 1162, 1163, 1164, 1165, 1172, 1173,\n",
    "                          1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "                          1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "                          1197, 1198, 1199, 1200, 1201]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Plotting -- we will need to query the schemas generated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Template(\"\"\"\n",
    "WITH mg_rns AS (\n",
    "  SELECT *,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY base_value DESC, base_max_recall_ratio ASC, RANDOM()) AS rn_base,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY adj_value DESC, adj_max_recall_ratio ASC, RANDOM()) AS rn_adj\n",
    "  FROM {{schema}}.model_adjustment_results_plevel\n",
    "  WHERE past_train_end_time = train_end_time\n",
    ")\n",
    ", base_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_base = 1\n",
    ")\n",
    ", adj_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_adj = 1\n",
    ")\n",
    "\n",
    "-- Simple model selection on last time period, baseline with no recall adjustments\n",
    "SELECT 'Best Unadjusted Metric - Unadjusted'::VARCHAR(128) AS strategy,\n",
    "       r.train_end_time, r.past_train_end_time,\n",
    "       r.list_size, r.metric, r.parameter,\n",
    "       r.base_value AS value,\n",
    "       r.base_max_recall_ratio AS max_recall_ratio,\n",
    "       r.base_recall_highest_to_not_highest AS recall_high_to_not_high,\n",
    "       r.base_recall_not_highest_to_highest AS recall_not_high_to_high\n",
    "FROM {{schema}}.model_adjustment_results_plevel r\n",
    "JOIN base_mgs b\n",
    "  ON r.model_group_id = b.model_group_id\n",
    "  AND r.past_train_end_time = b.train_end_time\n",
    "  AND r.list_size = b.list_size\n",
    "  AND r.metric = b.metric\n",
    "  AND r.parameter = b.parameter\n",
    "WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Model selection on last time before adjustment, with adjustment applied\n",
    "SELECT 'Best Unadjusted Metric - Adjusted'::VARCHAR(128) AS strategy,\n",
    "       r.train_end_time, r.past_train_end_time,\n",
    "       r.list_size, r.metric, r.parameter,\n",
    "       r.adj_value AS value,\n",
    "       r.adj_max_recall_ratio AS max_recall_ratio,\n",
    "       r.adj_recall_highest_to_not_highest AS recall_high_to_not_high,\n",
    "       r.adj_recall_not_highest_to_highest AS recall_not_high_to_high\n",
    "FROM {{schema}}.model_adjustment_results_plevel r\n",
    "JOIN base_mgs b\n",
    "  ON r.model_group_id = b.model_group_id\n",
    "  AND r.past_train_end_time = b.train_end_time\n",
    "  AND r.list_size = b.list_size\n",
    "  AND r.metric = b.metric\n",
    "  AND r.parameter = b.parameter\n",
    "WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Model selection on last time after adjustment, with adjustment applied\n",
    "SELECT 'Best Adjusted Metric - Adjusted'::VARCHAR(128) AS strategy,\n",
    "       r.train_end_time, r.past_train_end_time,\n",
    "       r.list_size, r.metric, r.parameter,\n",
    "       r.adj_value AS value,\n",
    "       r.adj_max_recall_ratio AS max_recall_ratio,\n",
    "       r.adj_recall_highest_to_not_highest AS recall_high_to_not_high,\n",
    "       r.adj_recall_not_highest_to_highest AS recall_not_high_to_high\n",
    "FROM {{schema}}.model_adjustment_results_plevel r\n",
    "JOIN adj_mgs b\n",
    "  ON r.model_group_id = b.model_group_id\n",
    "  AND r.past_train_end_time = b.train_end_time\n",
    "  AND r.list_size = b.list_size\n",
    "  AND r.metric = b.metric\n",
    "  AND r.parameter = b.parameter\n",
    "WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Composite model\n",
    "SELECT 'Composite Model - Adjusted'::VARCHAR(128) AS strategy,\n",
    "      r.train_end_time AS train_end_time,\n",
    "      r.past_train_end_time AS past_train_end_time,\n",
    "      r.list_size, metric, parameter,\n",
    "      r.value AS value,\n",
    "      r.max_recall_ratio AS max_recall_ratio,\n",
    "      r.recall_highest_to_not_highest AS recall_high_to_not_high,\n",
    "      r.recall_not_highest_to_highest AS recall_not_high_to_high\n",
    "FROM {{schema}}.composite_results_plevel r\n",
    "WHERE train_end_time > past_train_end_time;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_decoupled = Template(\"\"\"\n",
    "WITH mg_rns AS (\n",
    "  SELECT *,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY base_value DESC, base_max_recall_ratio ASC, RANDOM()) AS rn_base,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY adj_value DESC, adj_max_recall_ratio ASC, RANDOM()) AS rn_adj\n",
    "  FROM {{schema}}.model_adjustment_results_plevel\n",
    "  WHERE past_train_end_time = train_end_time\n",
    ")\n",
    ", base_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_base = 1\n",
    ")\n",
    ", adj_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_adj = 1\n",
    ")\n",
    "\n",
    "SELECT 'Composite w/ Decoupled - Adjusted'::VARCHAR(128) AS strategy,\n",
    "      train_end_time, past_train_end_time,\n",
    "      list_size, metric, parameter,\n",
    "      value,\n",
    "      max_recall_ratio,\n",
    "      recall_highest_to_not_highest AS recall_high_to_not_high,\n",
    "      recall_not_highest_to_highest AS recall_not_high_to_high\n",
    "FROM {{schema}}.composite_results_decoupled_plevel\n",
    "WHERE train_end_time > past_train_end_time\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['schema'] = 'hemank_bias_aug24_original'\n",
    "sql_orig = query.render(**params)\n",
    "ts_df_orig = pd.read_sql(sql_orig, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'hemank_bias_aug24_nop'\n",
    "sql_nop = query.render(**params)\n",
    "ts_df_nop = pd.read_sql(sql_nop, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_decoupled'\n",
    "sql_decoupled = query_decoupled.render(**params)\n",
    "ts_df_decoupled = pd.read_sql(sql_decoupled, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_zafar'\n",
    "sql_zafar = query.render(**params)\n",
    "ts_df_zafar = pd.read_sql(sql_zafar, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['schema'] = 'donors_bias_under_1_orig_orig'\n",
    "sql_u_v1a = query.render(**params)\n",
    "ts_df_u_v1a = pd.read_sql(sql_u_v1a, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_under_orig_50_50'\n",
    "sql_u_v2a = query.render(**params)\n",
    "ts_df_u_v2a = pd.read_sql(sql_u_v2a, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_under_orig_snop_orig'\n",
    "sql_u_v2b = query.render(**params)\n",
    "ts_df_u_v2b = pd.read_sql(sql_u_v2b, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_under_orig_50_orig'\n",
    "sql_u_v2c = query.render(**params)\n",
    "ts_df_u_v2c = pd.read_sql(sql_u_v2c, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_under_1_50_50'\n",
    "sql_u_v3a = query.render(**params)\n",
    "ts_df_u_v3a = pd.read_sql(sql_u_v3a, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_under_1_snop_orig'\n",
    "sql_u_v3b = query.render(**params)\n",
    "ts_df_u_v3b = pd.read_sql(sql_u_v3b, conn)\n",
    "\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_over_1_orig_orig'\n",
    "sql_o_v1a = query.render(**params)\n",
    "ts_df_o_v1a = pd.read_sql(sql_o_v1a, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_over_orig_50_50'\n",
    "sql_o_v2a = query.render(**params)\n",
    "ts_df_o_v2a = pd.read_sql(sql_o_v2a, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_over_orig_snop_orig'\n",
    "sql_o_v2b = query.render(**params)\n",
    "ts_df_o_v2b = pd.read_sql(sql_o_v2b, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_over_orig_50_orig'\n",
    "sql_o_v2c = query.render(**params)\n",
    "ts_df_o_v2c = pd.read_sql(sql_o_v2c, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_over_1_50_50'\n",
    "sql_o_v3a = query.render(**params)\n",
    "ts_df_o_v3a = pd.read_sql(sql_o_v3a, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_over_1_snop_orig'\n",
    "sql_o_v3b = query.render(**params)\n",
    "ts_df_o_v3b = pd.read_sql(sql_o_v3b, conn)\n",
    "\n",
    "params = {}\n",
    "params['schema'] = 'donors_bias_zafar'\n",
    "sql_zafar = query.render(**params)\n",
    "ts_df_zafar = pd.read_sql(sql_zafar, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df_orig['dataset'] = 'Original'\n",
    "orig_df = ts_df_orig.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_nop['dataset'] = 'NoP'\n",
    "nop_df = ts_df_nop.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_decoupled['dataset'] = 'Decoupled'\n",
    "decoupled_df = ts_df_decoupled.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns'\n",
    ")\n",
    "\n",
    "ts_df_u_v1a['dataset'] = 'Under-v1a'\n",
    "u_v1a_df = ts_df_u_v1a.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v2a['dataset'] = 'Under-v2a'\n",
    "u_v2a_df = ts_df_u_v2a.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v2b['dataset'] = 'Under-v2b'\n",
    "u_v2b_df = ts_df_u_v2b.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v2c['dataset'] = 'Under-v2c'\n",
    "u_v2c_df = ts_df_u_v2c.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v3a['dataset'] = 'Under-v3a'\n",
    "u_v3a_df = ts_df_u_v3a.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v3b['dataset'] = 'Under-v3b'\n",
    "u_v3b_df = ts_df_u_v3b.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v1a['dataset'] = 'Over-v1a'\n",
    "o_v1a_df = ts_df_o_v1a.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v2a['dataset'] = 'Over-v2a'\n",
    "o_v2a_df = ts_df_o_v2a.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v2b['dataset'] = 'Over-v2b'\n",
    "o_v2b_df = ts_df_o_v2b.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v2c['dataset'] = 'Over-v2c'\n",
    "o_v2c_df = ts_df_o_v2c.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v3a['dataset'] = 'Over-v3a'\n",
    "o_v3a_df = ts_df_o_v3a.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v3b['dataset'] = 'Over-v3b'\n",
    "o_v3b_df = ts_df_o_v3b.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_zafar['dataset'] = 'Zafar'\n",
    "zafar_df = ts_df_zafar.rename(\n",
    "    {'recall_not_high_to_high': 'recall_disp'\n",
    "    }, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_two(comp_df):\n",
    "    #colorlist = sns.color_palette(\"dark\", 12).as_hex()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    #data = comp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()\n",
    "    data = comp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reindex([\"Original\", \"No Protected\", \n",
    "                                                                                        \"Sampling (Best Equity)\",\n",
    "                                                                                       \"Sampling (Best Precision)\",\n",
    "                                                                                       \"Composite Adjusted\",\n",
    "                                                                                       \"Regularization\", \n",
    "                                                                                  \"Post-hoc Adjustment\"]).reset_index()\n",
    "\n",
    "    unique_dfs = [\"Original\", \"No Protected\", \"Sampling (Best Equity)\", \"Sampling (Best Precision)\",\n",
    "                 \"Composite Adjusted\", \"Regularization\", \"Post-hoc Adjustment\"]\n",
    "    \n",
    "    colorlist = ['#001c7f', #Original\n",
    "                 '#b1400d', #No_Protected\n",
    "                 '#12711c', #Best Sampling(Equity)\n",
    "                 #'#8c0800', #OLD -Best Sampling(Precision)\n",
    "                 '#7CFC00', #NEW - Best Sampling(Precision)\n",
    "                 '#591e71', #Composite Adjustment\n",
    "                 '#592f0d', #Regularization\n",
    "                 '#a23582' #Post-hoc Adjustment\n",
    "                ]\n",
    "    \n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'dataset',\n",
    "                 markers = True,\n",
    "                 palette = colorlist,\n",
    "                 linewidth = 20.0,\n",
    "                 marker = \"o\",\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    #unique_dfs = np.unique(data['dataset'])\n",
    "    \n",
    "    for i in range(len(unique_dfs)):\n",
    "        print(\"Doing for:\"+str(unique_dfs[i])+\" with color=\"+str(colorlist[i]))\n",
    "        tmp_df = comp_df.loc[comp_df['dataset'] == unique_dfs[i], ].copy()\n",
    "        \n",
    "        x_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['dataset'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['dataset'])['recall_disp'].sem().values\n",
    "        \n",
    "        color = colorlist[i]\n",
    "        \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = colorlist[i], fmt= ' ', zorder=-1, capsize=5, linewidth=3.0)\n",
    "        \n",
    "    #ax.set_ylabel('Recall Disparity', fontsize=20)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=28)\n",
    "    ax.plot([0.0, 1.0], [1.0, 1.0], linestyle='--', color='black')\n",
    "    ax.tick_params(axis='x', labelsize=22)\n",
    "    ax.tick_params(axis='y', labelsize=24)\n",
    "    ax.get_legend().remove()\n",
    "    \n",
    "    ax.set_xlim([0.45, 0.6])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig_two_df = pd.concat([orig_df, nop_df, zafar_df, decoupled_df])\n",
    "\n",
    "filtered_original_df = orig_df[(orig_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "filtered_original_df.loc[:,'dataset'] = 'Original'\n",
    "filtered_original_df.loc[:,'strategy'] = 'No Adjustment'\n",
    "\n",
    "filtered_adjusted_df = orig_df[(orig_df['strategy'] == 'Best Unadjusted Metric - Adjusted')]\n",
    "filtered_adjusted_df.loc[:,'dataset'] = 'Post-hoc Adjustment'\n",
    "filtered_adjusted_df.loc[:,'strategy'] = 'Post-hoc Adjustment'\n",
    "\n",
    "filtered_composite_df = orig_df[(orig_df['strategy'] == 'Composite Model - Adjusted')]\n",
    "filtered_composite_df.loc[:,'dataset'] = 'Composite Adjusted'\n",
    "filtered_composite_df.loc[:,'strategy'] = 'Composite Adjusted'\n",
    "\n",
    "filtered_nop_df = nop_df[nop_df['strategy'] == 'Best Unadjusted Metric - Unadjusted']\n",
    "filtered_nop_df.loc[:,'dataset'] = 'No Protected'\n",
    "filtered_nop_df.loc[:,'strategy'] = 'No Protected'\n",
    "\n",
    "filtered_zafar_df = zafar_df[zafar_df['strategy'] == 'Best Unadjusted Metric - Unadjusted']\n",
    "filtered_zafar_df.loc[:,'dataset'] = 'Regularization'\n",
    "filtered_zafar_df.loc[:,'strategy'] = 'Regularization'\n",
    "\n",
    "best_prec_df = u_v3b_df[u_v3b_df['strategy'] == 'Best Unadjusted Metric - Unadjusted']\n",
    "best_prec_df.loc[:,'dataset'] = 'Sampling (Best Precision)'\n",
    "best_prec_df.loc[:,'strategy'] = 'Sampling (Best Precision)'\n",
    "\n",
    "best_equity_df = o_v2a_df[o_v2a_df['strategy'] == 'Best Unadjusted Metric - Unadjusted']\n",
    "best_equity_df.loc[:,'dataset'] = 'Sampling (Best Equity)'\n",
    "best_equity_df.loc[:,'strategy'] = 'Sampling (Best Equity)'\n",
    "\n",
    "fig_two_df = pd.concat([filtered_original_df, filtered_nop_df, filtered_zafar_df, \n",
    "                        best_prec_df, best_equity_df, filtered_adjusted_df,\n",
    "                       filtered_composite_df])\n",
    "\n",
    "fig_three_df = pd.concat([filtered_original_df, filtered_nop_df])\n",
    "fig_four_df = pd.concat([filtered_original_df, filtered_zafar_df])\n",
    "\n",
    "filtered_decoupled_df = decoupled_df[(decoupled_df['strategy'] == 'Composite w/ Decoupled - Adjusted')]\n",
    "filtered_decoupled_df.loc[:,'strategy'] = 'Decoupled'\n",
    "filtered_decoupled_df.loc[:,'dataset'] = 'Decoupled'\n",
    "\n",
    "fig_five_df = pd.concat([filtered_original_df, filtered_composite_df, filtered_decoupled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure_two(fig_two_df)\n",
    "plt.savefig('../../PLOTS2/PLOTS_Summary/Donors_Method_NEW.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_three(comp_df):\n",
    "    colorlist = sns.color_palette(\"dark\", 12).as_hex()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data = comp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reindex([\"Original\", \"No Protected\"]\n",
    "                                                                                ).reset_index()\n",
    "    colorlist = ['#001c7f', #Original\n",
    "                 '#b1400d']\n",
    "    unique_dfs = [\"Original\", \"No Protected\"]\n",
    "    \n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'dataset',\n",
    "                 markers = True,\n",
    "                 palette = colorlist,\n",
    "                 marker = \"o\",\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    unique_dfs = [\"Original\", \"No Protected\"]\n",
    "        \n",
    "    for i in range(len(unique_dfs)):\n",
    "        tmp_df = comp_df.loc[comp_df['dataset'] == unique_dfs[i], ].copy()\n",
    "        \n",
    "        x_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['dataset'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['dataset'])['recall_disp'].sem().values\n",
    "        \n",
    "        color = colorlist[i]\n",
    "        \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = colorlist[i], fmt= ' ', zorder=-1, capsize=5, linewidth=3.0)\n",
    "        \n",
    "    ax.set_ylabel('', fontsize=32)\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=28)\n",
    "    ax.plot([0.0,1.0],[1.0, 1.0], linestyle = '--', color='black')\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=24)\n",
    "    ax.tick_params(axis='y', labelsize=24)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    hhandles = []\n",
    "    llabels = []\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        #if lab not in list(unique_dfs):# + ['dataset']:\n",
    "        #    handles[i].set_linestyle(\"\")\n",
    "        if lab not in list(unique_dfs):\n",
    "            handles[i].set_linestyle(\"\")\n",
    "            lab = \"Methods\"\n",
    "        hhandles.append(handles[i])\n",
    "        llabels.append(lab)\n",
    "\n",
    "    ax.legend(hhandles[1:], llabels[1:], fontsize=24, \n",
    "              bbox_to_anchor=(0.015, 0.95), \n",
    "              loc='upper left', \n",
    "              borderaxespad=0., markerscale=2, ncol=1)\n",
    "\n",
    "    ax.set_xlim([0.45, 0.58])\n",
    "    ax.set_ylim([0.85, 2.2])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure_three(fig_three_df)\n",
    "plt.savefig('../../PLOTS2/PLOTS_NOP/NoP_Donors.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_four(comp_df):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data = comp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reindex([\"Original\", \"Regularization\"]\n",
    "                                                                                ).reset_index()\n",
    "    colorlist = ['#001c7f', '#592f0d'] #Regularization\n",
    "    \n",
    "    unique_dfs = [\"Original\", \"Regularization\"]\n",
    "\n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'dataset',\n",
    "                 markers = True,\n",
    "                 palette = colorlist,\n",
    "                 linewidth = 20.0,\n",
    "                 marker = \"o\",\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    for i in range(len(unique_dfs)):\n",
    "        tmp_df = comp_df.loc[comp_df['dataset'] == unique_dfs[i], ].copy()        \n",
    "        x_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['dataset'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['dataset'])['recall_disp'].sem().values\n",
    "        \n",
    "        color = colorlist[i]\n",
    "    \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = colorlist[i], fmt= ' ', zorder=-1, capsize=5, linewidth=3.0)\n",
    "        \n",
    "    ax.set_ylabel('', fontsize=32)\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=28)\n",
    "    ax.plot([0.0,1.0],[1.0, 1.0], linestyle = '--', color='black')\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=24)\n",
    "    ax.tick_params(axis='y', labelsize=24)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    hhandles = []\n",
    "    llabels = []\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        #if lab not in list(unique_dfs):# + ['dataset']:\n",
    "        #    handles[i].set_linestyle(\"\")\n",
    "        if lab not in list(unique_dfs):\n",
    "            handles[i].set_linestyle(\"\")\n",
    "            lab = \"Methods\"\n",
    "        hhandles.append(handles[i])\n",
    "        llabels.append(lab)\n",
    "\n",
    "    ax.legend(hhandles[1:], llabels[1:], fontsize=24, \n",
    "              bbox_to_anchor=(0.020, 0.125), \n",
    "              loc='lower left', \n",
    "              borderaxespad=0., markerscale=2, ncol=1)\n",
    "    \n",
    "    ax.set_ylim([0.85, 2.2])\n",
    "    ax.set_xlim([0.5, 0.6])\n",
    "    ax.plot([0.5, 0.6], [1.0, 1.0], color='black', linestyle='--')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure_four(fig_four_df)\n",
    "plt.savefig('../../PLOTS2/PLOTS_Zafar/donors_zafar.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_five(comp_df):\n",
    "    colorlist = sns.color_palette(\"dark\", 12).as_hex()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data = comp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reindex([\"Original\", \"Composite Adjusted\", \"Decoupled\"]\n",
    "                                                                                ).reset_index()\n",
    "\n",
    "    colorlist = ['#001c7f', #Original\n",
    "                 '#591e71', # Composite Adjustment\n",
    "                 '#3c3c3c' # Decoupled\n",
    "                ]\n",
    "\n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'dataset',\n",
    "                 markers = True,\n",
    "                 palette = colorlist,\n",
    "                 marker = \"o\",\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    unique_dfs = [\"Original\", \"Composite Adjusted\", \"Decoupled\"]\n",
    "    \n",
    "    for i in range(len(unique_dfs)):\n",
    "        tmp_df = comp_df.loc[comp_df['dataset'] == unique_dfs[i], ].copy()\n",
    "        \n",
    "        x_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['dataset'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['dataset'])['recall_disp'].sem().values\n",
    "        \n",
    "        color = colorlist[i]\n",
    "        \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = colorlist[i], fmt= ' ', zorder=-1, capsize=5)\n",
    "        \n",
    "    ax.set_ylabel('', fontsize=32)\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=28)\n",
    "    ax.plot([0.0,1.0],[1.0, 1.0], linestyle = '--', color='black')\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=24)\n",
    "    ax.tick_params(axis='y', labelsize=24)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    hhandles = []\n",
    "    llabels = []\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        #if lab not in list(unique_dfs):# + ['dataset']:\n",
    "        #    handles[i].set_linestyle(\"\")\n",
    "        if lab not in list(unique_dfs):\n",
    "            handles[i].set_linestyle(\"\")\n",
    "            lab = \"Methods\"\n",
    "        hhandles.append(handles[i])\n",
    "        llabels.append(lab)\n",
    "\n",
    "    ax.legend(hhandles[1:], llabels[1:], fontsize=20, \n",
    "              bbox_to_anchor=(0.0125, 0.225), \n",
    "              loc='lower left', \n",
    "              borderaxespad=0., markerscale=2, ncol=1)\n",
    "    \n",
    "    ax.set_ylim([0.85, 2.3])\n",
    "    ax.set_xlim([0.48, 0.62])\n",
    "    ax.plot([0.1, 0.9], [1.0, 1.0], color='black', linestyle='--')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure_five(fig_five_df)\n",
    "plt.savefig('../../PLOTS2/PLOTS_Composite/Composite_Donors.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(x):\n",
    "    if \"Original\" in x:\n",
    "        return \"Original\"\n",
    "    elif \"U\" in x:\n",
    "        return \"Under\"\n",
    "    return \"Over\"\n",
    "\n",
    "usamp_df = pd.concat([u_v1a_df, u_v2a_df, u_v2b_df, u_v2c_df, u_v3a_df, u_v3b_df])\n",
    "osamp_df = pd.concat([o_v1a_df, o_v2a_df, o_v2b_df, o_v2c_df, o_v3a_df, o_v3b_df])\n",
    "\n",
    "filtered_orig_df = orig_df[(orig_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "filtered_usamp_df = usamp_df[(usamp_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "filtered_osamp_df = osamp_df[(osamp_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "\n",
    "fig_six_df = pd.concat([filtered_orig_df, filtered_usamp_df, filtered_osamp_df])\n",
    "fig_six_df.loc[fig_six_df['strategy']=='Best Unadjusted Metric - Unadjusted', 'strategy'] = 'No Adjustment'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Original', 'mode'] = 'Original'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v1a', 'mode'] = 'Under'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v1a', 'dataset'] = '1'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v2a', 'mode'] = 'Under'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v2a', 'dataset'] = '2'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v2b', 'mode'] = 'Under'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v2b', 'dataset'] = '3'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v2c', 'mode'] = 'Under'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v2c', 'dataset'] = '4'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v3a', 'mode'] = 'Under'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v3a', 'dataset'] = '5'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v3b', 'mode'] = 'Under'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Under-v3b', 'dataset'] = '6'\n",
    "\n",
    "fig_six_df.loc[fig_six_df['strategy']=='Best Unadjusted Metric - Unadjusted', 'strategy'] = 'No Adjustment'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v1a', 'mode'] = 'Over'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v1a', 'dataset'] = '1'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v2a', 'mode'] = 'Over'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v2a', 'dataset'] = '2'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v2b', 'mode'] = 'Over'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v2b', 'dataset'] = '3'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v2c', 'mode'] = 'Over'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v2c', 'dataset'] = '4'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v3a', 'mode'] = 'Over'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v3a', 'dataset'] = '5'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v3b', 'mode'] = 'Over'\n",
    "fig_six_df.loc[fig_six_df['dataset']=='Over-v3b', 'dataset'] = '6'\n",
    "\n",
    "fig_six_df = fig_six_df.rename(columns={\"dataset\": \"Method\", \"mode\":\"Mode\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_six_df.groupby(['Method', 'Mode'])[['value', 'recall_disp']].mean().reindex([\n",
    "        (\"Original\", \"Original\"),\n",
    "        (\"1\", \"Under\"), (\"2\", \"Under\"), (\"3\", \"Under\"),\n",
    "        (\"4\", \"Under\"), (\"5\", \"Under\"), (\"6\", \"Under\"), \n",
    "        (\"1\", \"Over\"), (\"2\", \"Over\"), (\"3\", \"Over\"), \n",
    "        (\"4\", \"Over\"), (\"5\", \"Over\"), (\"6\", \"Over\")]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_six(comp_df):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    colorlist = ['#001c7f', #Original\n",
    "                 '#12711c', \n",
    "                 #'#23fab9', \n",
    "                 #'#00875e',\n",
    "                 #'#3fb9c4',\n",
    "                 '#a2ff00']\n",
    "                 #'#688c29']\n",
    "    \n",
    "    data = comp_df.groupby(['Method', 'Mode'])[['value', 'recall_disp']].mean().reindex([\n",
    "        (\"Original\", \"Original\"),\n",
    "        (\"1\", \"Under\"), (\"2\", \"Under\"), (\"3\", \"Under\"),\n",
    "        #(\"4\", \"Under\"), \n",
    "        (\"5\", \"Under\"), (\"6\", \"Under\"), \n",
    "        (\"1\", \"Over\"), (\"2\", \"Over\"), (\"3\", \"Over\"), \n",
    "        #(\"4\", \"Over\"),\n",
    "        (\"5\", \"Over\"), (\"6\", \"Over\")]).reset_index()\n",
    "    \n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'Mode',\n",
    "                 style = 'Method',\n",
    "                 markers = True,\n",
    "                 palette = colorlist,\n",
    "                 color = colorlist,\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    unique_dfs = [(\"Original\",\"Original\"),\n",
    "                  (\"1\",\"Under\"), (\"1\",\"Over\"),\n",
    "                  (\"2\",\"Under\"),(\"2\",\"Over\"),\n",
    "                  (\"3\",\"Under\"),(\"3\",\"Over\"),\n",
    "                  #(\"4\",\"Under\"),(\"4\",\"Over\"),\n",
    "                  (\"5\",\"Under\"),(\"5\",\"Over\"),\n",
    "                  (\"6\",\"Under\"),(\"6\",\"Over\")]\n",
    "    \n",
    "    \n",
    "    for i in range(len(unique_dfs)):\n",
    "        tmp_df = comp_df.loc[(comp_df['Method'] == unique_dfs[i][0]) & (comp_df['Mode']==unique_dfs[i][1]), ].copy()\n",
    "        \n",
    "        x_coords = list(tmp_df.groupby(['Method', 'Mode'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['Method', 'Mode'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['Method', 'Mode'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['Method', 'Mode'])['recall_disp'].sem().values\n",
    "    \n",
    "        mode = unique_dfs[i][1]\n",
    "        if mode == \"Original\":\n",
    "            color = colorlist[0]\n",
    "        elif mode == \"Under\":\n",
    "            color = colorlist[1]\n",
    "        else:\n",
    "            color = colorlist[2]\n",
    "            \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = color, fmt= ' ', zorder=-1, capsize=5)\n",
    "        \n",
    "    ax.set_ylabel('', fontsize=32)\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=28)\n",
    "    ax.plot([0.0,1.0],[1.0, 1.0], linestyle = '--', color='black')\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=24)\n",
    "    ax.tick_params(axis='y', labelsize=24)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    hhandles = []\n",
    "    llabels = []\n",
    "\n",
    "    #for i, lab in enumerate(labels):\n",
    "    #    if lab not in list(unique_dfs):\n",
    "    #        handles[i].set_linestyle(\"\")\n",
    "    #        lab = \"Methods\"\n",
    "    #    hhandles.append(handles[i])\n",
    "    #    llabels.append(lab)\n",
    "\n",
    "    ax.legend(handles, labels, fontsize=18, \n",
    "              bbox_to_anchor=(0.015, 0.995), \n",
    "              loc='upper left', \n",
    "              borderaxespad=0., markerscale=2, ncol=1)\n",
    "    \n",
    "    ax.plot([0.1, 0.9], [1.0, 1.0], color='black', linestyle='--')\n",
    "    ax.set_xlim([0.45, 0.58])\n",
    "    ax.set_ylim([0.95, 2.5])\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figure_six(fig_six_df)\n",
    "plt.savefig('../../PLOTS2/PLOTS_Sampling/Donors_Sampling.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_seven_df = pd.concat([filtered_orig_df, filtered_adjusted_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure_seven(comp_df):\n",
    "    colorlist = sns.color_palette(\"dark\", 12).as_hex()\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    data = comp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reindex([\"Original\", \"Post-hoc Adjustment\"]\n",
    "                                                                                ).reset_index()\n",
    "\n",
    "    colorlist = ['#001c7f', #Original\n",
    "                '#a23582']\n",
    "\n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'dataset',\n",
    "                 markers = True,\n",
    "                 palette = colorlist,\n",
    "                 marker = \"o\",\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    unique_dfs = [\"Original\", \"Post-hoc Adjustment\"]\n",
    "    \n",
    "    for i in range(len(unique_dfs)):\n",
    "        tmp_df = comp_df.loc[comp_df['dataset'] == unique_dfs[i], ].copy()\n",
    "        \n",
    "        x_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['dataset'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['dataset'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['dataset'])['recall_disp'].sem().values\n",
    "        \n",
    "        color = colorlist[i]\n",
    "        \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = colorlist[i], fmt= ' ', zorder=-1, capsize=5)\n",
    "        \n",
    "    ax.set_ylabel('', fontsize=32)\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=28)\n",
    "    ax.plot([0.0,1.0],[1.0, 1.0], linestyle = '--', color='black')\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=24)\n",
    "    ax.tick_params(axis='y', labelsize=24)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    hhandles = []\n",
    "    llabels = []\n",
    "\n",
    "    for i, lab in enumerate(labels):\n",
    "        #if lab not in list(unique_dfs):# + ['dataset']:\n",
    "        #    handles[i].set_linestyle(\"\")\n",
    "        if lab not in list(unique_dfs):\n",
    "            handles[i].set_linestyle(\"\")\n",
    "            lab = \"Methods\"\n",
    "        hhandles.append(handles[i])\n",
    "        llabels.append(lab)\n",
    "\n",
    "    llabels = ['Methods', 'Original', 'Post-hoc\\nAdjustment']\n",
    "    ax.legend(hhandles[1:], llabels[1:], fontsize=20, \n",
    "              bbox_to_anchor=(0.01, 0.985), \n",
    "              loc='upper left', \n",
    "              borderaxespad=0., markerscale=2, ncol=1)\n",
    "\n",
    "    ax.set_xlim([0.5, 0.6])\n",
    "    ax.set_ylim([0.9, 2.3])\n",
    "    ax.plot([0.1, 0.9], [1.0, 1.0], color='black', linestyle='--')\n",
    "    #ax.legend().remove()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_one_df = pd.concat([orig_df, nop_df, zafar_df, decoupled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_orig_df = orig_df[(orig_df['strategy'] == 'Best Unadjusted Metric - Unadjusted') \n",
    "                           | (orig_df['strategy'] == 'Best Unadjusted Metric - Adjusted')\n",
    "                          | (orig_df['strategy'] == 'Composite Model - Adjusted')\n",
    "                          | (orig_df['strategy'] == 'Composite Model w/ Decoupled - Adjusted')]\n",
    "\n",
    "filtered_nop_df = nop_df[nop_df['strategy'] == 'Best Unadjusted Metric - Unadjusted']\n",
    "filtered_zafar_df = zafar_df[zafar_df['strategy'] == 'Best Unadjusted Metric - Unadjusted']\n",
    "\n",
    "best_usamp_prec_df = u_v1a_df[(u_v1a_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "best_usamp_prec_df.loc[:,'dataset'] = 'Best UnderSample (Value)'\n",
    "best_usamp_equity_df = u_v3b_df[(u_v3b_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "best_usamp_equity_df.loc[:,'dataset'] = 'Best UnderSample (Equity)'\n",
    "\n",
    "best_osamp_prec_df = o_v1a_df[(o_v1a_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "best_osamp_prec_df.loc[:,'dataset'] = 'Best OverSample (Value)'\n",
    "best_osamp_equity_df = o_v2a_df[(o_v2a_df['strategy'] == 'Best Unadjusted Metric - Unadjusted')]\n",
    "best_osamp_equity_df.loc[:,'dataset'] = 'Best OverSample (Equity)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_one_df = pd.concat([filtered_orig_df, filtered_nop_df, filtered_zafar_df, best_usamp_prec_df, \n",
    "                       best_usamp_equity_df, best_osamp_prec_df, best_osamp_equity_df, decoupled_df])\n",
    "\n",
    "fig_one_df.loc[fig_one_df['strategy']=='Best Unadjusted Metric - Unadjusted', 'strategy'] = 'No Adjustment'\n",
    "fig_one_df.loc[fig_one_df['strategy']=='Best Unadjusted Metric - Adjusted', 'strategy'] = 'Adjusted'\n",
    "fig_one_df.loc[fig_one_df['strategy']=='Composite w/ Decoupled - Adjusted','strategy'] = 'Decoupled Adjusted'\n",
    "fig_one_df.loc[fig_one_df['strategy']=='Composite Model - Adjusted','strategy'] = 'Composite Adjusted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(fig_one_df)\n",
    "plt.savefig('../../PLOTS2/Donors_Summary.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
