{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemank/.pyenv/versions/3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:17: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display \n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline\n",
    "\n",
    "def connect(poolclass=sqlalchemy.pool.QueuePool):\n",
    "    #with open(os.path.join(os.path.join('../..','config'), 'joco_db_profile.yaml')) as fd:\n",
    "    #with open(os.path.join(os.path.join('../..','config'), 'donors_db_profile.yaml')) as fd:\n",
    "    with open(os.path.join(os.path.join('../..','config'), 'san_jose_db.yaml')) as fd:\n",
    "        config = yaml.load(fd)\n",
    "        dburl = sqlalchemy.engine.url.URL(\n",
    "            \"postgres\",\n",
    "            host=config[\"host\"],\n",
    "            username=config[\"user\"],\n",
    "            database=config[\"db\"],\n",
    "            password=config[\"pass\"],\n",
    "            port=config[\"port\"],\n",
    "        )\n",
    "        return sqlalchemy.create_engine(dburl, poolclass=poolclass)\n",
    "\n",
    "    \n",
    "conn = connect()\n",
    "\n",
    "import SAVE_RecallAdjuster as sra\n",
    "from importlib import reload\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import seaborn as sns\n",
    "\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>model_group_id</th>\n",
       "      <th>run_time</th>\n",
       "      <th>model_type</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>model_comment</th>\n",
       "      <th>train_end_time</th>\n",
       "      <th>train_matrix_uuid</th>\n",
       "      <th>test_matrix_uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 21:59:39.055658</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-06-01</td>\n",
       "      <td>b25675417e8dd0e136535533f3f11e60</td>\n",
       "      <td>6e8e810b80ac3af7098f3620312a741d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 21:59:45.308385</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>010d727303f5d32260241fd306717a77</td>\n",
       "      <td>662a16643753e2c5ece08225ac1b5c4d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 21:59:52.363199</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>54290fe03c1c5072e28bc6ec556ce01b</td>\n",
       "      <td>d4a54921a9b8f8a91397a2b7a60c33e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 21:59:59.874882</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-03-01</td>\n",
       "      <td>b2a4e7a9d02ec872f15012e504507777</td>\n",
       "      <td>96abfd5193109b66a0a5153ee63a2a49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:00:08.444466</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-06-01</td>\n",
       "      <td>dccae7c7f3716a97e81ac9ee8d3d0ce3</td>\n",
       "      <td>adcce78f4026913a901035b8826f204e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:00:18.443976</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>3f2c4f740b5c1d68615197f8c3736721</td>\n",
       "      <td>0a151833f8e68a5337e012731d876bce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:00:29.120892</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>8c99383600e32c924a4a3936342be64a</td>\n",
       "      <td>9596fcbf2240e278820588a8ac4ecab5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:00:40.928045</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>b0fa1bd8831bb3b2c47b5700b42ff354</td>\n",
       "      <td>7fb31341eb6ca578618e5bb91b66d014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:00:54.580170</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-06-01</td>\n",
       "      <td>bc94da6ef87d8cf31368fcaca45eb53c</td>\n",
       "      <td>7d6491d1a4901cd2f53a4bc7f47776ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:01:09.918113</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>19e907e593a5bd01bb5f20f0a6f37d99</td>\n",
       "      <td>63df835e0f63c31d8d2ed9d7294018ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:01:26.406172</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-12-01</td>\n",
       "      <td>e6e6c4e6de865eb5f11a161070b85de2</td>\n",
       "      <td>c1997adb65ecf2c052dc7a8836ad4b1c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:01:45.318638</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-03-01</td>\n",
       "      <td>4bd2035c37d7401b0706d5db54d74b75</td>\n",
       "      <td>9acd7da2c9c116bca4a14dc271602534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:02:05.230723</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>b80a7052e7e4103bac0c79169b792c7b</td>\n",
       "      <td>5834a059245d3204dd2dff72b28e2ac0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:02:26.748616</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>638a38ae3403e2942c73551d389abe25</td>\n",
       "      <td>5aa2c3c3b4f21f5f3b35e3832995c172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:02:50.728816</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>081c8b737841b969fe4187a6e5839c45</td>\n",
       "      <td>3849ebcdd46e7f4b33d3b98bb02f8ca3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:03:15.530325</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>699e051fbd830e65feb200243bffbdb8</td>\n",
       "      <td>e627cfc8728fb7e5e0df32e46a21ad67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:03:42.608187</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>eb72a4ba6d3a403350c5afd26467bf14</td>\n",
       "      <td>efd3da6ff2cb3767f9d2e172640dd431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:04:11.826183</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-09-01</td>\n",
       "      <td>1bb7e52e0782dab8ca6caed057bc46a3</td>\n",
       "      <td>15811b6296b3c37d480cefd912e72dcb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:04:43.947011</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>ab5731a8a42ad34974714987d6de7263</td>\n",
       "      <td>4ea82edbfc64775e48de8f0bd4f78c3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:05:18.105978</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-03-01</td>\n",
       "      <td>89421ac8c0b4b9d454268311faade0ea</td>\n",
       "      <td>0483b395f7384152a2a905fe81decdfa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>9999</td>\n",
       "      <td>2021-02-21 22:05:55.841014</td>\n",
       "      <td>zafar_regularized</td>\n",
       "      <td>{'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>9b4ea3b76e5b8246ee93b82e06f2f077</td>\n",
       "      <td>fa4829351962e3aa87b4f164b25852c2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_id  model_group_id                   run_time         model_type  \\\n",
       "0          0            9999 2021-02-21 21:59:39.055658  zafar_regularized   \n",
       "1          1            9999 2021-02-21 21:59:45.308385  zafar_regularized   \n",
       "2          2            9999 2021-02-21 21:59:52.363199  zafar_regularized   \n",
       "3          3            9999 2021-02-21 21:59:59.874882  zafar_regularized   \n",
       "4          4            9999 2021-02-21 22:00:08.444466  zafar_regularized   \n",
       "5          5            9999 2021-02-21 22:00:18.443976  zafar_regularized   \n",
       "6          6            9999 2021-02-21 22:00:29.120892  zafar_regularized   \n",
       "7          7            9999 2021-02-21 22:00:40.928045  zafar_regularized   \n",
       "8          8            9999 2021-02-21 22:00:54.580170  zafar_regularized   \n",
       "9          9            9999 2021-02-21 22:01:09.918113  zafar_regularized   \n",
       "10        10            9999 2021-02-21 22:01:26.406172  zafar_regularized   \n",
       "11        11            9999 2021-02-21 22:01:45.318638  zafar_regularized   \n",
       "12        12            9999 2021-02-21 22:02:05.230723  zafar_regularized   \n",
       "13        13            9999 2021-02-21 22:02:26.748616  zafar_regularized   \n",
       "14        14            9999 2021-02-21 22:02:50.728816  zafar_regularized   \n",
       "15        15            9999 2021-02-21 22:03:15.530325  zafar_regularized   \n",
       "16        16            9999 2021-02-21 22:03:42.608187  zafar_regularized   \n",
       "17        17            9999 2021-02-21 22:04:11.826183  zafar_regularized   \n",
       "18        18            9999 2021-02-21 22:04:43.947011  zafar_regularized   \n",
       "19        19            9999 2021-02-21 22:05:18.105978  zafar_regularized   \n",
       "20        20            9999 2021-02-21 22:05:55.841014  zafar_regularized   \n",
       "\n",
       "                                      hyperparameters model_comment  \\\n",
       "0   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "1   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "2   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "3   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "4   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "5   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "6   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "7   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "8   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "9   {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "10  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "11  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "12  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "13  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "14  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "15  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "16  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "17  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "18  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "19  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "20  {'mu': 1.2, 'EPS': '1e-4', 'tau': 0.005, 'max_...          None   \n",
       "\n",
       "   train_end_time                 train_matrix_uuid  \\\n",
       "0      2011-06-01  b25675417e8dd0e136535533f3f11e60   \n",
       "1      2011-09-01  010d727303f5d32260241fd306717a77   \n",
       "2      2011-12-01  54290fe03c1c5072e28bc6ec556ce01b   \n",
       "3      2012-03-01  b2a4e7a9d02ec872f15012e504507777   \n",
       "4      2012-06-01  dccae7c7f3716a97e81ac9ee8d3d0ce3   \n",
       "5      2012-09-01  3f2c4f740b5c1d68615197f8c3736721   \n",
       "6      2012-12-01  8c99383600e32c924a4a3936342be64a   \n",
       "7      2013-03-01  b0fa1bd8831bb3b2c47b5700b42ff354   \n",
       "8      2013-06-01  bc94da6ef87d8cf31368fcaca45eb53c   \n",
       "9      2013-09-01  19e907e593a5bd01bb5f20f0a6f37d99   \n",
       "10     2013-12-01  e6e6c4e6de865eb5f11a161070b85de2   \n",
       "11     2014-03-01  4bd2035c37d7401b0706d5db54d74b75   \n",
       "12     2014-06-01  b80a7052e7e4103bac0c79169b792c7b   \n",
       "13     2014-09-01  638a38ae3403e2942c73551d389abe25   \n",
       "14     2014-12-01  081c8b737841b969fe4187a6e5839c45   \n",
       "15     2015-03-01  699e051fbd830e65feb200243bffbdb8   \n",
       "16     2015-06-01  eb72a4ba6d3a403350c5afd26467bf14   \n",
       "17     2015-09-01  1bb7e52e0782dab8ca6caed057bc46a3   \n",
       "18     2015-12-01  ab5731a8a42ad34974714987d6de7263   \n",
       "19     2016-03-01  89421ac8c0b4b9d454268311faade0ea   \n",
       "20     2016-06-01  9b4ea3b76e5b8246ee93b82e06f2f077   \n",
       "\n",
       "                    test_matrix_uuid  \n",
       "0   6e8e810b80ac3af7098f3620312a741d  \n",
       "1   662a16643753e2c5ece08225ac1b5c4d  \n",
       "2   d4a54921a9b8f8a91397a2b7a60c33e4  \n",
       "3   96abfd5193109b66a0a5153ee63a2a49  \n",
       "4   adcce78f4026913a901035b8826f204e  \n",
       "5   0a151833f8e68a5337e012731d876bce  \n",
       "6   9596fcbf2240e278820588a8ac4ecab5  \n",
       "7   7fb31341eb6ca578618e5bb91b66d014  \n",
       "8   7d6491d1a4901cd2f53a4bc7f47776ae  \n",
       "9   63df835e0f63c31d8d2ed9d7294018ab  \n",
       "10  c1997adb65ecf2c052dc7a8836ad4b1c  \n",
       "11  9acd7da2c9c116bca4a14dc271602534  \n",
       "12  5834a059245d3204dd2dff72b28e2ac0  \n",
       "13  5aa2c3c3b4f21f5f3b35e3832995c172  \n",
       "14  3849ebcdd46e7f4b33d3b98bb02f8ca3  \n",
       "15  e627cfc8728fb7e5e0df32e46a21ad67  \n",
       "16  efd3da6ff2cb3767f9d2e172640dd431  \n",
       "17  15811b6296b3c37d480cefd912e72dcb  \n",
       "18  4ea82edbfc64775e48de8f0bd4f78c3b  \n",
       "19  0483b395f7384152a2a905fe81decdfa  \n",
       "20  fa4829351962e3aa87b4f164b25852c2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_sql(\"select * from triage_metadata.zafar_models\", conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zafar FOR San Jose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2011-03-01', '2011-03-01'), ('2011-03-01', '2012-03-01'), ('2011-06-01', '2011-06-01'), ('2011-06-01', '2012-06-01'), ('2011-09-01', '2011-09-01'), ('2011-09-01', '2012-09-01'), ('2011-12-01', '2011-12-01'), ('2011-12-01', '2012-12-01'), ('2012-03-01', '2012-03-01'), ('2012-03-01', '2013-03-01'), ('2012-06-01', '2012-06-01'), ('2012-06-01', '2013-06-01'), ('2012-09-01', '2012-09-01'), ('2012-09-01', '2013-09-01'), ('2012-12-01', '2012-12-01'), ('2012-12-01', '2013-12-01'), ('2013-03-01', '2013-03-01'), ('2013-03-01', '2014-03-01'), ('2013-06-01', '2013-06-01'), ('2013-06-01', '2014-06-01'), ('2013-09-01', '2013-09-01'), ('2013-09-01', '2014-09-01'), ('2013-12-01', '2013-12-01'), ('2013-12-01', '2014-12-01'), ('2014-03-01', '2014-03-01'), ('2014-03-01', '2015-03-01'), ('2014-06-01', '2014-06-01'), ('2014-06-01', '2015-06-01'), ('2014-09-01', '2014-09-01'), ('2014-09-01', '2015-09-01'), ('2014-12-01', '2014-12-01'), ('2014-12-01', '2015-12-01'), ('2015-03-01', '2015-03-01'), ('2015-03-01', '2016-03-01'), ('2015-06-01', '2015-06-01'), ('2015-06-01', '2016-06-01')]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "base = datetime.datetime.strptime('2016-06-01', '%Y-%m-%d')\n",
    "date_pairs = []\n",
    "for x in range(17,-1,-1):\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=3*x) - relativedelta(years=1)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=3*x) - relativedelta(years=1)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=3*x) - relativedelta(years=1)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=3*x)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "print(date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SET ROLE postgres;\n",
      "\n",
      "CREATE SCHEMA IF NOT EXISTS joco_zafar;\n",
      "\n",
      "-- ASSUMPTIONS:\n",
      "--    1. every model for a given train_end_time uses the same cohort\n",
      "\n",
      "\n",
      "DROP TABLE IF EXISTS joco_zafar.tmp_bias_end_times;\n",
      "CREATE TABLE joco_zafar.tmp_bias_end_times\n",
      "  AS\n",
      "  SELECT UNNEST(ARRAY['2011-03-01','2011-03-01','2011-06-01','2011-06-01','2011-09-01','2011-09-01','2011-12-01','2011-12-01','2012-03-01','2012-03-01','2012-06-01','2012-06-01','2012-09-01','2012-09-01','2012-12-01','2012-12-01','2013-03-01','2013-03-01','2013-06-01','2013-06-01','2013-09-01','2013-09-01','2013-12-01','2013-12-01','2014-03-01','2014-03-01','2014-06-01','2014-06-01','2014-09-01','2014-09-01','2014-12-01','2014-12-01','2015-03-01','2015-03-01','2015-06-01','2015-06-01'])::TIMESTAMP AS past_train_end_time,\n",
      "         UNNEST(ARRAY['2011-03-01','2012-03-01','2011-06-01','2012-06-01','2011-09-01','2012-09-01','2011-12-01','2012-12-01','2012-03-01','2013-03-01','2012-06-01','2013-06-01','2012-09-01','2013-09-01','2012-12-01','2013-12-01','2013-03-01','2014-03-01','2013-06-01','2014-06-01','2013-09-01','2014-09-01','2013-12-01','2014-12-01','2014-03-01','2015-03-01','2014-06-01','2015-06-01','2014-09-01','2015-09-01','2014-12-01','2015-12-01','2015-03-01','2016-03-01','2015-06-01','2016-06-01'])::TIMESTAMP AS future_train_end_time\n",
      ";\n",
      "\n",
      "DROP TABLE IF EXISTS joco_zafar.tmp_bias_list_sizes;\n",
      "CREATE TABLE joco_zafar.tmp_bias_list_sizes\n",
      "  AS\n",
      "  SELECT UNNEST(ARRAY[500])::INT AS list_size\n",
      ";\n",
      "\n",
      "DROP TABLE IF EXISTS joco_zafar.tmp_bias_models;\n",
      "CREATE TABLE joco_zafar.tmp_bias_models\n",
      "  AS\n",
      "  WITH all_end_times AS (\n",
      "    SELECT DISTINCT past_train_end_time AS train_end_time FROM joco_zafar.tmp_bias_end_times\n",
      "    UNION DISTINCT\n",
      "    SELECT DISTINCT future_train_end_time AS train_end_time FROM joco_zafar.tmp_bias_end_times\n",
      "  )\n",
      "  SELECT DISTINCT zm.model_id, zm.model_group_id, zm.train_end_time\n",
      "  FROM triage_metadata.zafar_models zm\n",
      "  JOIN triage_metadata.zafar_model_groups zmg USING(model_group_id)\n",
      "  JOIN all_end_times USING(train_end_time)\n",
      "  WHERE zm.model_id >= 0\n",
      "  AND zm.model_id <= 20\n",
      ";\n",
      "ALTER TABLE joco_zafar.tmp_bias_models ADD PRIMARY KEY (model_id);\n",
      "\n",
      "Not executing Entity Demos -- picking up from previously built schema.\n",
      "Done creating entities\n",
      "Setting demo values. Not using pre-set values\n",
      "Not running subsampling OR bootstrap based models\n",
      "Running Recall Adjustment\n",
      "Time Taken=2.5970346927642822\n"
     ]
    }
   ],
   "source": [
    "reload(sra)\n",
    "start_time = time.time()\n",
    "myRA_zafar = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='postgres',\n",
    "        schema='bias_zafar',\n",
    "        experiment_hashes='None',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[500],\n",
    "        entity_demos='kit_bias_adj.entity_demos',\n",
    "        demo_col='median_income',\n",
    "        start_model_id = 0,\n",
    "        end_model_id = 20\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running for Joco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2014-04-01', '2014-04-01'), ('2014-04-01', '2015-04-01'), ('2014-08-01', '2014-08-01'), ('2014-08-01', '2015-08-01'), ('2014-12-01', '2014-12-01'), ('2014-12-01', '2015-12-01'), ('2015-04-01', '2015-04-01'), ('2015-04-01', '2016-04-01'), ('2015-08-01', '2015-08-01'), ('2015-08-01', '2016-08-01'), ('2015-12-01', '2015-12-01'), ('2015-12-01', '2016-12-01'), ('2016-04-01', '2016-04-01'), ('2016-04-01', '2017-04-01'), ('2016-08-01', '2016-08-01'), ('2016-08-01', '2017-08-01'), ('2016-12-01', '2016-12-01'), ('2016-12-01', '2017-12-01'), ('2017-04-01', '2017-04-01'), ('2017-04-01', '2018-04-01')]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "base = datetime.datetime.strptime('2018-04-01', '%Y-%m-%d')\n",
    "date_pairs = []\n",
    "for x in range(9,-1,-1):\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=4*x) - relativedelta(years=1)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=4*x) - relativedelta(years=1)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=4*x) - relativedelta(years=1)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=4*x)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "\n",
    "import seaborn as sns\n",
    "print(date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SET ROLE hemank;\n",
      "\n",
      "CREATE SCHEMA IF NOT EXISTS joco_zafar;\n",
      "\n",
      "-- ASSUMPTIONS:\n",
      "--    1. every model for a given train_end_time uses the same cohort\n",
      "\n",
      "\n",
      "DROP TABLE IF EXISTS joco_zafar.tmp_bias_end_times;\n",
      "CREATE TABLE joco_zafar.tmp_bias_end_times\n",
      "  AS\n",
      "  SELECT UNNEST(ARRAY['2014-04-01','2014-04-01','2014-08-01','2014-08-01','2014-12-01','2014-12-01','2015-04-01','2015-04-01','2015-08-01','2015-08-01','2015-12-01','2015-12-01','2016-04-01','2016-04-01','2016-08-01','2016-08-01','2016-12-01','2016-12-01','2017-04-01','2017-04-01'])::TIMESTAMP AS past_train_end_time,\n",
      "         UNNEST(ARRAY['2014-04-01','2015-04-01','2014-08-01','2015-08-01','2014-12-01','2015-12-01','2015-04-01','2016-04-01','2015-08-01','2016-08-01','2015-12-01','2016-12-01','2016-04-01','2017-04-01','2016-08-01','2017-08-01','2016-12-01','2017-12-01','2017-04-01','2018-04-01'])::TIMESTAMP AS future_train_end_time\n",
      ";\n",
      "\n",
      "DROP TABLE IF EXISTS joco_zafar.tmp_bias_list_sizes;\n",
      "CREATE TABLE joco_zafar.tmp_bias_list_sizes\n",
      "  AS\n",
      "  SELECT UNNEST(ARRAY[500])::INT AS list_size\n",
      ";\n",
      "\n",
      "DROP TABLE IF EXISTS joco_zafar.tmp_bias_models;\n",
      "CREATE TABLE joco_zafar.tmp_bias_models\n",
      "  AS\n",
      "  WITH all_end_times AS (\n",
      "    SELECT DISTINCT past_train_end_time AS train_end_time FROM joco_zafar.tmp_bias_end_times\n",
      "    UNION DISTINCT\n",
      "    SELECT DISTINCT future_train_end_time AS train_end_time FROM joco_zafar.tmp_bias_end_times\n",
      "  )\n",
      "  SELECT DISTINCT zm.model_id, zm.model_group_id, zm.train_end_time\n",
      "  FROM model_metadata.zafar_models zm\n",
      "  JOIN model_metadata.zafar_model_groups zmg USING(model_group_id)\n",
      "  JOIN all_end_times USING(train_end_time)\n",
      "  WHERE zm.model_id >= 0\n",
      "  AND zm.model_id <= 12\n",
      ";\n",
      "ALTER TABLE joco_zafar.tmp_bias_models ADD PRIMARY KEY (model_id);\n",
      "\n",
      "Not executing Entity Demos -- picking up from previously built schema.\n",
      "Done creating entities\n",
      "Setting demo values. Not using pre-set values\n",
      "Not running subsampling OR bootstrap based models\n",
      "Running Recall Adjustment\n",
      "Time Taken=7.3101396560668945\n"
     ]
    }
   ],
   "source": [
    "reload(sra)\n",
    "start_time = time.time()\n",
    "myRA_zafar = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='joco_zafar',\n",
    "        experiment_hashes='None',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[500],\n",
    "        entity_demos='hemank_bias_alternatives.currmatch_entity_demos',\n",
    "        demo_col='race_2way',\n",
    "        start_model_id = 0,\n",
    "        end_model_id = 12\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2010-05-01', '2010-05-01'), ('2010-05-01', '2010-07-01'), ('2010-07-01', '2010-07-01'), ('2010-07-01', '2010-09-01'), ('2010-09-01', '2010-09-01'), ('2010-09-01', '2010-11-01'), ('2010-11-01', '2010-11-01'), ('2010-11-01', '2011-01-01'), ('2011-01-01', '2011-01-01'), ('2011-01-01', '2011-03-01'), ('2011-03-01', '2011-03-01'), ('2011-03-01', '2011-05-01'), ('2011-05-01', '2011-05-01'), ('2011-05-01', '2011-07-01'), ('2011-07-01', '2011-07-01'), ('2011-07-01', '2011-09-01'), ('2011-09-01', '2011-09-01'), ('2011-09-01', '2011-11-01'), ('2011-11-01', '2011-11-01'), ('2011-11-01', '2012-01-01'), ('2012-01-01', '2012-01-01'), ('2012-01-01', '2012-03-01'), ('2012-03-01', '2012-03-01'), ('2012-03-01', '2012-05-01'), ('2012-05-01', '2012-05-01'), ('2012-05-01', '2012-07-01'), ('2012-07-01', '2012-07-01'), ('2012-07-01', '2012-09-01'), ('2012-09-01', '2012-09-01'), ('2012-09-01', '2012-11-01'), ('2012-11-01', '2012-11-01'), ('2012-11-01', '2013-01-01'), ('2013-01-01', '2013-01-01'), ('2013-01-01', '2013-03-01')]\n"
     ]
    }
   ],
   "source": [
    "base = datetime.datetime.strptime('2013-03-01', '%Y-%m-%d')   #Corresponding to latest train_end_time\\n\",\n",
    "date_pairs = []\n",
    "for x in range(16, -1, -1):\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=2*x) - relativedelta(months=2)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=2*x) - relativedelta(months=2)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=2*x) - relativedelta(months=2)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=2*x)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "print(date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SET ROLE hemank;\n",
      "\n",
      "CREATE SCHEMA IF NOT EXISTS donors_bias_zafar;\n",
      "\n",
      "-- ASSUMPTIONS:\n",
      "--    1. every model for a given train_end_time uses the same cohort\n",
      "\n",
      "\n",
      "DROP TABLE IF EXISTS donors_bias_zafar.tmp_bias_end_times;\n",
      "CREATE TABLE donors_bias_zafar.tmp_bias_end_times\n",
      "  AS\n",
      "  SELECT UNNEST(ARRAY['2010-05-01','2010-05-01','2010-07-01','2010-07-01','2010-09-01','2010-09-01','2010-11-01','2010-11-01','2011-01-01','2011-01-01','2011-03-01','2011-03-01','2011-05-01','2011-05-01','2011-07-01','2011-07-01','2011-09-01','2011-09-01','2011-11-01','2011-11-01','2012-01-01','2012-01-01','2012-03-01','2012-03-01','2012-05-01','2012-05-01','2012-07-01','2012-07-01','2012-09-01','2012-09-01','2012-11-01','2012-11-01','2013-01-01','2013-01-01'])::TIMESTAMP AS past_train_end_time,\n",
      "         UNNEST(ARRAY['2010-05-01','2010-07-01','2010-07-01','2010-09-01','2010-09-01','2010-11-01','2010-11-01','2011-01-01','2011-01-01','2011-03-01','2011-03-01','2011-05-01','2011-05-01','2011-07-01','2011-07-01','2011-09-01','2011-09-01','2011-11-01','2011-11-01','2012-01-01','2012-01-01','2012-03-01','2012-03-01','2012-05-01','2012-05-01','2012-07-01','2012-07-01','2012-09-01','2012-09-01','2012-11-01','2012-11-01','2013-01-01','2013-01-01','2013-03-01'])::TIMESTAMP AS future_train_end_time\n",
      ";\n",
      "\n",
      "DROP TABLE IF EXISTS donors_bias_zafar.tmp_bias_list_sizes;\n",
      "CREATE TABLE donors_bias_zafar.tmp_bias_list_sizes\n",
      "  AS\n",
      "  SELECT UNNEST(ARRAY[1000])::INT AS list_size\n",
      ";\n",
      "\n",
      "DROP TABLE IF EXISTS donors_bias_zafar.tmp_bias_models;\n",
      "CREATE TABLE donors_bias_zafar.tmp_bias_models\n",
      "  AS\n",
      "  WITH all_end_times AS (\n",
      "    SELECT DISTINCT past_train_end_time AS train_end_time FROM donors_bias_zafar.tmp_bias_end_times\n",
      "    UNION DISTINCT\n",
      "    SELECT DISTINCT future_train_end_time AS train_end_time FROM donors_bias_zafar.tmp_bias_end_times\n",
      "  )\n",
      "  SELECT DISTINCT zm.model_id, zm.model_group_id, zm.train_end_time\n",
      "  FROM model_metadata.zafar_models zm\n",
      "  JOIN model_metadata.zafar_model_groups zmg USING(model_group_id)\n",
      "  JOIN all_end_times USING(train_end_time)\n",
      "  WHERE zm.model_id >= 0\n",
      "  AND zm.model_id <= 17\n",
      ";\n",
      "ALTER TABLE donors_bias_zafar.tmp_bias_models ADD PRIMARY KEY (model_id);\n",
      "\n",
      "Not executing Entity Demos -- picking up from previously built schema.\n",
      "Done creating entities\n",
      "Setting demo values. Not using pre-set values\n",
      "Not running subsampling OR bootstrap based models\n",
      "Running Recall Adjustment\n",
      "Time Taken=20.068074703216553\n"
     ]
    }
   ],
   "source": [
    "reload(sra)\n",
    "start_time = time.time()\n",
    "myRA_zafar = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='donors_bias_zafar',\n",
    "        experiment_hashes='None',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[1000],\n",
    "        entity_demos='hemank_bias_original.entity_demos3',\n",
    "        demo_col='plevel',\n",
    "        start_model_id = 0,\n",
    "        end_model_id = 17\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Template(\"\"\"\n",
    "WITH mg_rns AS (\n",
    "  SELECT *,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY base_value DESC, base_max_recall_ratio ASC, RANDOM()) AS rn_base,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY adj_value DESC, adj_max_recall_ratio ASC, RANDOM()) AS rn_adj\n",
    "  FROM {{schema}}.model_adjustment_results_race_2way\n",
    "  WHERE past_train_end_time = train_end_time\n",
    ")\n",
    ", base_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_base = 1\n",
    ")\n",
    ", adj_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_adj = 1\n",
    ")\n",
    "\n",
    "-- Simple model selection on last time period, baseline with no recall adjustments\n",
    "SELECT 'Best Unadjusted Metric - Unadjusted'::VARCHAR(128) AS strategy,\n",
    "       r.train_end_time, r.past_train_end_time,\n",
    "       r.list_size, r.metric, r.parameter,\n",
    "       r.base_value AS value,\n",
    "       r.base_max_recall_ratio AS max_recall_ratio,\n",
    "       r.base_recall_white_to_nonwhite AS recall_w_to_nw,\n",
    "       r.base_recall_nonwhite_to_white AS recall_nw_to_w\n",
    "FROM {{schema}}.model_adjustment_results_race_2way r\n",
    "JOIN base_mgs b\n",
    "  ON r.model_group_id = b.model_group_id\n",
    "  AND r.past_train_end_time = b.train_end_time\n",
    "  AND r.list_size = b.list_size\n",
    "  AND r.metric = b.metric\n",
    "  AND r.parameter = b.parameter\n",
    "WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Model selection on last time before adjustment, with adjustment applied\n",
    "SELECT 'Best Unadjusted Metric - Adjusted'::VARCHAR(128) AS strategy,\n",
    "       r.train_end_time, r.past_train_end_time,\n",
    "       r.list_size, r.metric, r.parameter,\n",
    "       r.adj_value AS value,\n",
    "       r.adj_max_recall_ratio AS max_recall_ratio,\n",
    "       r.adj_recall_white_to_nonwhite AS recall_w_to_nw,\n",
    "       r.adj_recall_nonwhite_to_white AS recall_nw_to_w\n",
    "FROM {{schema}}.model_adjustment_results_race_2way r\n",
    "JOIN base_mgs b\n",
    "  ON r.model_group_id = b.model_group_id\n",
    "  AND r.past_train_end_time = b.train_end_time\n",
    "  AND r.list_size = b.list_size\n",
    "  AND r.metric = b.metric\n",
    "  AND r.parameter = b.parameter\n",
    "WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Model selection on last time after adjustment, with adjustment applied\n",
    "SELECT 'Best Adjusted Metric - Adjusted'::VARCHAR(128) AS strategy,\n",
    "       r.train_end_time, r.past_train_end_time,\n",
    "       r.list_size, r.metric, r.parameter,\n",
    "       r.adj_value AS value,\n",
    "       r.adj_max_recall_ratio AS max_recall_ratio,\n",
    "       r.adj_recall_white_to_nonwhite AS recall_w_to_nw,\n",
    "       r.adj_recall_nonwhite_to_white AS recall_nw_to_w\n",
    "FROM {{schema}}.model_adjustment_results_race_2way r\n",
    "JOIN adj_mgs b\n",
    "  ON r.model_group_id = b.model_group_id\n",
    "  AND r.past_train_end_time = b.train_end_time\n",
    "  AND r.list_size = b.list_size\n",
    "  AND r.metric = b.metric\n",
    "  AND r.parameter = b.parameter\n",
    "WHERE r.train_end_time > r.past_train_end_time\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "-- Composite model\n",
    "SELECT 'Composite Model - Adjusted'::VARCHAR(128) AS strategy,\n",
    "      r.train_end_time AS train_end_time,\n",
    "      r.past_train_end_time AS past_train_end_time,\n",
    "      r.list_size, metric, parameter,\n",
    "      r.value AS value,\n",
    "      r.max_recall_ratio AS max_recall_ratio,\n",
    "      r.recall_white_to_nonwhite AS recall_w_to_nw,\n",
    "      r.recall_nonwhite_to_white AS recall_nw_to_w\n",
    "FROM {{schema}}.composite_results_race_2way r\n",
    "WHERE train_end_time > past_train_end_time\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_decoupled = Template(\"\"\"\n",
    "WITH mg_rns AS (\n",
    "  SELECT *,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY base_value DESC, base_max_recall_ratio ASC, RANDOM()) AS rn_base,\n",
    "         row_number() OVER (PARTITION BY train_end_time, list_size, metric, parameter ORDER BY adj_value DESC, adj_max_recall_ratio ASC, RANDOM()) AS rn_adj\n",
    "  FROM {{schema}}.model_adjustment_results_race_2way\n",
    "  WHERE past_train_end_time = train_end_time\n",
    ")\n",
    ", base_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_base = 1\n",
    ")\n",
    ", adj_mgs AS (\n",
    "  SELECT * FROM mg_rns WHERE rn_adj = 1\n",
    ")\n",
    "\n",
    "\n",
    "-- -- Composite model (with decoupled models)\n",
    "SELECT 'Composite w/ Decoupled - Adjusted'::VARCHAR(128) AS strategy,\n",
    "      train_end_time, past_train_end_time,\n",
    "      list_size, metric, parameter,\n",
    "      value,\n",
    "      max_recall_ratio,\n",
    "      recall_white_to_nonwhite AS recall_w_to_nw,\n",
    "      recall_nonwhite_to_white AS recall_nw_to_w\n",
    "FROM {{schema}}.composite_results_decoupled_race_2way\n",
    "WHERE train_end_time > past_train_end_time\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['schema'] = 'joco_bias_original'\n",
    "sql_orig = query.render(**params)\n",
    "ts_df_orig = pd.read_sql(sql_orig, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_nop'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_nop = pd.read_sql(sql_us, conn)\n",
    "\n",
    "params['schema'] = 'joco_zafar'\n",
    "sql_zafar = query.render(**params)\n",
    "ts_df_zafar = pd.read_sql(sql_zafar, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_decoupled'\n",
    "sql_decoupled = query_decoupled.render(**params)\n",
    "ts_df_decoupled = pd.read_sql(sql_decoupled, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling\n",
    "params = {}\n",
    "params['schema'] = 'joco_bias_under_1_orig_orig'\n",
    "sql_orig = query.render(**params)\n",
    "ts_df_u_v1a = pd.read_sql(sql_orig, conn)\n",
    "\n",
    "#params['schema'] = 'joco_bias_under_2_orig_orig'\n",
    "#sql_us = query.render(**params)\n",
    "#ts_df_u_v1b = pd.read_sql(sql_us, conn)\n",
    "\n",
    "#params['schema'] = 'joco_bias_under_3_orig_orig'\n",
    "#sql_us = query.render(**params)\n",
    "#ts_df_u_v1c = pd.read_sql(sql_us, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_under_orig_50_50'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_u_v2a = pd.read_sql(sql_us, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_under_orig_50_orig'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_u_v2c = pd.read_sql(sql_us, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_under_orig_same_nop_orig'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_u_v2b = pd.read_sql(sql_us, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_under_1_50_50'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_u_v3a = pd.read_sql(sql_us, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_under_1_same_nop_orig'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_u_v3b = pd.read_sql(sql_us, conn)\n",
    "\n",
    "ts_df_u_v1a['dataset'] = 'Under-v1a'\n",
    "u_v1a_df = ts_df_u_v1a.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "#ts_df_u_v1b['dataset'] = 'Under-v1b'\n",
    "#u_v1b_df = ts_df_u_v1b.rename(\n",
    "#    {'recall_w_to_nw': 'recall_disp', \n",
    "#     'frac_white': 'frac_grp1', \n",
    "#     'frac_nonwhite': 'frac_grp2'\n",
    "#    }, axis='columns')\n",
    "\n",
    "\n",
    "#ts_df_u_v1c['dataset'] = 'Under-v1c'\n",
    "#u_v1c_df = ts_df_u_v1c.rename(\n",
    "#    {'recall_w_to_nw': 'recall_disp', \n",
    "#     'frac_white': 'frac_grp1', \n",
    "#     'frac_nonwhite': 'frac_grp2'\n",
    "#    }, axis='columns')\n",
    "\n",
    "ts_df_u_v2a['dataset'] = 'Under-v2a'\n",
    "u_v2a_df = ts_df_u_v2a.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v2b['dataset'] = 'Under-v2b'\n",
    "u_v2b_df = ts_df_u_v2b.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v2c['dataset'] = 'Under-v2c'\n",
    "u_v2c_df = ts_df_u_v2c.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v3a['dataset'] = 'Under-v3a'\n",
    "u_v3a_df = ts_df_u_v3a.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_u_v3b['dataset'] = 'Under-v3b'\n",
    "u_v3b_df = ts_df_u_v3b.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling\n",
    "params = {}\n",
    "params['schema'] = 'joco_bias_over_1_orig_orig'\n",
    "sql_os = query.render(**params)\n",
    "ts_df_o_v1a = pd.read_sql(sql_os, conn)\n",
    "\n",
    "#params['schema'] = 'joco_bias_over_2_orig_orig'\n",
    "#sql_os = query.render(**params)\n",
    "#ts_df_o_v1b = pd.read_sql(sql_os, conn)\n",
    "\n",
    "#params['schema'] = 'joco_bias_over_3_orig_orig'\n",
    "#sql_os = query.render(**params)\n",
    "#ts_df_o_v1c = pd.read_sql(sql_os, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_over_orig_50_50'\n",
    "sql_os = query.render(**params)\n",
    "ts_df_o_v2a = pd.read_sql(sql_os, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_over_orig_50_orig'\n",
    "sql_os = query.render(**params)\n",
    "ts_df_o_v2c = pd.read_sql(sql_os, conn)\n",
    "\n",
    "#params['schema'] = 'joco_bias_over_orig_same_nop_orig'\n",
    "#sql_os = query.render(**params)\n",
    "#ts_df_o_v2b = pd.read_sql(sql_os, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_over_1_50_50'\n",
    "sql_os = query.render(**params)\n",
    "ts_df_o_v3a = pd.read_sql(sql_os, conn)\n",
    "\n",
    "params['schema'] = 'joco_bias_over_1_same_nop_orig'\n",
    "sql_os = query.render(**params)\n",
    "ts_df_o_v3b = pd.read_sql(sql_os, conn)\n",
    "\n",
    "ts_df_o_v1a['dataset'] = 'Over-v1a'\n",
    "o_v1a_df = ts_df_o_v1a.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "\n",
    "#ts_df_o_v1b['dataset'] = 'Over-v1b'\n",
    "#o_v1b_df = ts_df_o_v1b.rename(\n",
    "#    {'recall_w_to_nw': 'recall_disp', \n",
    "#     'frac_white': 'frac_grp1', \n",
    "#     'frac_nonwhite': 'frac_grp2'\n",
    "#    }, axis='columns')\n",
    "\n",
    "\n",
    "#ts_df_o_v1c['dataset'] = 'Over-v1c'\n",
    "#o_v1c_df = ts_df_o_v1c.rename(\n",
    "#    {'recall_w_to_nw': 'recall_disp', \n",
    "#     'frac_white': 'frac_grp1', \n",
    "#     'frac_nonwhite': 'frac_grp2'\n",
    "#    }, axis='columns')\n",
    "\n",
    "ts_df_o_v2a['dataset'] = 'Over-v2a'\n",
    "o_v2a_df = ts_df_o_v2a.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v2c['dataset'] = 'Over-v2c'\n",
    "o_v2c_df = ts_df_o_v2c.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "#ts_df_o_v2b['dataset'] = 'Over-v2b'\n",
    "#o_v2b_df = ts_df_o_v2b.rename(\n",
    "#    {'recall_w_to_nw': 'recall_disp', \n",
    "#     'frac_white': 'frac_grp1', \n",
    "#     'frac_nonwhite': 'frac_grp2'\n",
    "#    }, axis='columns')\n",
    "\n",
    "ts_df_o_v3a['dataset'] = 'Over-v3a'\n",
    "o_v3a_df = ts_df_o_v3a.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_o_v3b['dataset'] = 'Over-v3b'\n",
    "o_v3b_df = ts_df_o_v3b.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zafar_df = ts_df_zafar.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp',\n",
    "    'frac_white': 'frac_grp1',\n",
    "    'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "zafar_df['dataset'] = 'Zafar'\n",
    "\n",
    "ts_df_orig['dataset'] = 'Original'\n",
    "orig_df = ts_df_orig.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "ts_df_nop['dataset'] = 'NoP'\n",
    "nop_df = ts_df_nop.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp', \n",
    "     'frac_white': 'frac_grp1', \n",
    "     'frac_nonwhite': 'frac_grp2'\n",
    "    }, axis='columns')\n",
    "\n",
    "\n",
    "ts_df_decoupled['dataset'] = 'Decoupled'\n",
    "decoupled_df = ts_df_decoupled.rename(\n",
    "    {'recall_w_to_nw': 'recall_disp',\n",
    "     'frac_white': 'frac_grp1',\n",
    "     'frac_nonwhite':'frac_grp2'\n",
    "    }, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(zafar_df['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zafar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(nop_df['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsamp_df = pd.concat([orig_df, nop_df, zafar_df, decoupled_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsamp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usamp_df = pd.concat([orig_df, u_v1a_df, u_v2a_df, u_v2b_df, u_v2c_df, u_v3a_df, u_v3b_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "usamp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osamp_df = pd.concat([orig_df, o_v1a_df, o_v2a_df, o_v2c_df, o_v3a_df, o_v3b_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osamp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(comp_df):\n",
    "    colorlist = sns.color_palette(\"dark\", 12).as_hex()\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    data = comp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index()\n",
    "    \n",
    "    sns.lineplot(data = data,\n",
    "                 x='value', y='recall_disp',\n",
    "                 hue = 'dataset',\n",
    "                 style = 'strategy',\n",
    "                 markers = True,\n",
    "                 color = colorlist,\n",
    "                 markersize = 10,\n",
    "                 dashes = None,\n",
    "                 ci = None,\n",
    "                 ax = ax)\n",
    "    \n",
    "    unique_dfs = np.unique(data['dataset'])\n",
    "    \n",
    "    for i in range(len(unique_dfs)):\n",
    "        tmp_df = comp_df.loc[comp_df['dataset'] == unique_dfs[i], ].copy()\n",
    "        \n",
    "        x_coords = list(tmp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index()['value'].values)\n",
    "        y_coords = list(tmp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index()['recall_disp'].values)\n",
    "        \n",
    "        prec_errors = 1.96*tmp_df.groupby(['dataset', 'strategy'])['value'].sem().values\n",
    "        disp_errors = 1.96*tmp_df.groupby(['dataset', 'strategy'])['recall_disp'].sem().values\n",
    "        \n",
    "        color = colorlist[i]\n",
    "        \n",
    "        ax.errorbar(x_coords, y_coords,\n",
    "                   xerr = prec_errors,\n",
    "                   yerr = disp_errors,\n",
    "                   ecolor = colorlist[i], fmt= ' ', zorder=-1, capsize=5)\n",
    "        \n",
    "    ax.set_ylabel('Recall Disparity', fontsize=16)\n",
    "    ax.set_xlabel('Precision at Top-K', fontsize=16)\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=16)\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    \n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    hhandles = []\n",
    "    llabels = []\n",
    "    for i, lab in enumerate(labels):\n",
    "        if lab not in list(unique_dfs) + ['dataset', 'strategy']:\n",
    "            handles[i].set_linestyle(\"\")\n",
    "        hhandles.append(handles[i])\n",
    "        llabels.append(lab)\n",
    "        \n",
    "    ax.legend(hhandles, llabels, fontsize=16, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., markerscale=2)\n",
    "#    ax.set_xlim([0.5, 0.58])\n",
    "    fig.tight_layout()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(nonsamp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(usamp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(osamp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=comp_df.groupby(['dataset', 'strategy'])[['value', 'recall_disp']].mean().reset_index()\n",
    "mod_data = data[data['strategy']=='Best Unadjusted Metric - Unadjusted']\n",
    "\n",
    "mod_data = mod_data.grou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=mod_data.groupby(['dataset','strategy'])[['value', 'recall_disp']].mean().reset_index(), \n",
    "            x='value', y='recall_disp', \n",
    "              hue='dataset', \n",
    "              style='strategy',\n",
    "              markers=True,\n",
    "              color=colorlist,\n",
    "              markersize=10,\n",
    "              dashes=None,\n",
    "              ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_v2b_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Recall Adjuster on ZAFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SAVE_RecallAdjuster as sra\n",
    "from jinja2 import Template\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.dispose()\n",
    "conn = connect()\n",
    "reload(sra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "myRA_original = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='joco_bias_zafar',\n",
    "        experiment_hashes='',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[500],\n",
    "        entity_demos='hemank_bias_alternatives.currmatch_entity_demos',\n",
    "        demo_col='race_2way',\n",
    "        model_group_ids = 9999,\n",
    "        start_model_id = 0,\n",
    "        end_model_id = 14\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp - Running Zafar ONLY to see their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../fair-classification/disparate_mistreatment/synthetic_data_demo')\n",
    "sys.path.insert(0, '../fair-classification/fair_classification/')\n",
    "from generate_synthetic_data import *\n",
    "import utils as ut\n",
    "import funcs_disp_mist as fdm\n",
    "import plot_syn_boundaries as psb\n",
    "import pandas as pd\n",
    "import cvxpy\n",
    "import yaml\n",
    "from triage import create_engine\n",
    "from triage.component.catwalk.estimators.classifiers import ScaledLogisticRegression\n",
    "import time\n",
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_top_k(distances, k):\n",
    "    sort_df = pd.DataFrame({'dist': distances})\n",
    "    sort_df.sort_values('dist', ascending=False, inplace=True)\n",
    "    sort_df['pred_label'] = -1\n",
    "    sort_df['orig_idx'] = sort_df.index\n",
    "    sort_df.reset_index(inplace=True)\n",
    "    i = k-1\n",
    "    sort_df.loc[:i,'pred_label'] = 1\n",
    "    sort_df.sort_values('orig_idx', inplace=True)\n",
    "    \n",
    "    return sort_df['pred_label'].values\n",
    "\n",
    "\n",
    "def read_config_file(config_file):\n",
    "    config = None\n",
    "    try:\n",
    "        with open (config_file, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error reading the config file')\n",
    "    return config\n",
    "\n",
    "def connect(cred_folder):\n",
    "    cred_file = os.path.join(cred_folder, 'elsal_db_profile2.yaml')\n",
    "    db = read_config_file(cred_file)\n",
    "\n",
    "    sql_engine = create_engine(\n",
    "        'postgresql+psycopg2://%s:%s@%s:%i/%s'%(\n",
    "            db['user'],\n",
    "            db['pass'],\n",
    "            db['host'],\n",
    "            db['port'],\n",
    "            db['db']\n",
    "        )\n",
    "    )\n",
    "    return sql_engine\n",
    "\n",
    "def load_matrix(FILE_PATH, matrix_id, entity_to_attrib, demo_col, label_col):\n",
    "    df = pd.read_csv('%s/%s.csv.gz' % (FILE_PATH, matrix_id), compression='gzip')\n",
    "\n",
    "    entity_col = []\n",
    "    entities = df['entity_id'].values\n",
    "    \n",
    "    for i in range(len(entities)):\n",
    "        try:\n",
    "            attr = entity_to_attrib[int(entities[i])]\n",
    "            entity_col.append(attr)\n",
    "        except KeyError as e:\n",
    "            entity_col.append(\"MISSING\")\n",
    "\n",
    "    df[demo_col] = entity_col\n",
    "    df = df[df[demo_col]!='MISSING']\n",
    "    df[label_col] = 2*(df[label_col] - 0.5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_matrix_from_df(df, demo_col, label_col, cols_0):\n",
    "    exclude_cols = ['entity_id', 'as_of_date', label_col, demo_col]\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if c in cols_0:\n",
    "            exclude_cols.append(c)\n",
    "\n",
    "    x = df[[c for c in df.columns if c not in exclude_cols]].values\n",
    "    y = df[label_col].values\n",
    "    x_control = {demo_col:df[demo_col].values}\n",
    "\n",
    "    return x, y, x_control\n",
    "\n",
    "\n",
    "def get_train_test_matrix_pairs(engine, experiment_hash, model_group_id):\n",
    "    '''\n",
    "    args:\n",
    "        engine: PSQL Connection Engine\n",
    "        experiment_hash: use the relevant experiment_hash attributing to the train/test matrices you want to obtain\n",
    "        model_group_id: it might be better to use a model_group_id corresponding to \n",
    "        the dummy classifier for which train/test matrices might have been created; otherwise any relevant model_group_id\n",
    "        will do.\n",
    "    '''\n",
    "\n",
    "    query = \"\"\"\n",
    "    with rel_models as\n",
    "    (\n",
    "        select model_id, model_hash \n",
    "        from triage_metadata.models\n",
    "        where \n",
    "            built_by_experiment = '%s'\n",
    "            and model_group_id = %s\n",
    "    ),\n",
    "    train_matrices as\n",
    "    (\n",
    "        select model_id, matrix_uuid from\n",
    "        train_results.prediction_metadata\n",
    "    ),\n",
    "    test_matrices as \n",
    "    (\n",
    "        select model_id, matrix_uuid from \n",
    "        test_results.prediction_metadata\n",
    "    ),\n",
    "    matrix_info as \n",
    "    (\n",
    "        select matrix_id, matrix_uuid,\n",
    "        matrix_type, num_observations\n",
    "        from triage_metadata.matrices\n",
    "    )\n",
    "    select \n",
    "        rel_models.model_id, \n",
    "        train_matrices.matrix_uuid as train_matrix_id,\n",
    "        test_matrices.matrix_uuid as test_matrix_id,\n",
    "        m1.matrix_id as train_id, \n",
    "        m1.num_observations as train_n_obs,\n",
    "        m2.matrix_id as test_id,\n",
    "        m2.num_observations as test_n_obs\n",
    "    from \n",
    "        rel_models, \n",
    "        train_matrices, test_matrices,\n",
    "        matrix_info m1, matrix_info m2\n",
    "    where\n",
    "        rel_models.model_id = train_matrices.model_id \n",
    "    and\n",
    "        rel_models.model_id = test_matrices.model_id\n",
    "    and \n",
    "        m1.matrix_uuid = train_matrices.matrix_uuid\n",
    "    and \n",
    "        m2.matrix_uuid = test_matrices.matrix_uuid;\n",
    "    \"\"\"%(str(experiment_hash), str(model_group_id))\n",
    "\n",
    "    #print(query)\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(df.head(2))\n",
    "    \n",
    "    vals = df.values\n",
    "    train_test_matrices = []\n",
    "    \n",
    "    for v in vals:\n",
    "        train_test_matrices.append([(v[1], v[2])])\n",
    "\n",
    "    return train_test_matrices\n",
    "\n",
    "def get_entity_to_attrib_simple(conn, race_query, sa_column, anchor_sa_value):\n",
    "    entity_to_attrib = {}\n",
    "    df = pd.read_sql(race_query, conn)\n",
    "    \n",
    "    entity_ids = df['entity_id'].values\n",
    "    sa_vals = df[sa_column].values\n",
    "    \n",
    "    attrib_info = {}\n",
    "\n",
    "    for i in range(len(entity_ids)):\n",
    "        if(sa_vals[i]==anchor_sa_value):\n",
    "            sa_vals[i] = 1\n",
    "        else:\n",
    "            sa_vals[i] = 0.0\n",
    "        attrib_info[int(entity_ids[i])] = sa_vals[i]\n",
    "        \n",
    "    print(pd.value_counts(sa_vals))\n",
    "    return attrib_info\n",
    "\n",
    "def calc_prec(pred_label, actual_label):\n",
    "    label_pos = sum((pred_label == 1).astype(int))\n",
    "    true_pos = sum(np.logical_and(pred_label == actual_label, pred_label == 1).astype(int))\n",
    "    return float(true_pos/label_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect('../../config')\n",
    "experiment_hash='a1316f404aecc9df9e3c5264b32770f8'\n",
    "race_query='select entity_id, ovg from bias_analysis_1year.entity_demos'\n",
    "label_col='dropout'\n",
    "demo_col='ovg'\n",
    "model_group_id=201\n",
    "sa_column='ovg'\n",
    "anchor_sa_value='1'\n",
    "file_path='/mnt/data/experiment_data/elsal_new/original/matrices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model_id                   train_matrix_id  \\\n",
      "0      5307  f2fc585d0b29ba9d0795284cde71617c   \n",
      "1      5362  23792b71e95b894495c002d7d5904a41   \n",
      "\n",
      "                     test_matrix_id  \\\n",
      "0  de82f83ec6fdb91ffa32cbd7dccb8dc6   \n",
      "1  e75531dc8f63533890425ccd8b9b8613   \n",
      "\n",
      "                                            train_id  train_n_obs  \\\n",
      "0  dropout_binary_2009-01-01 00:00:00_2010-01-01 ...            0   \n",
      "1  dropout_binary_2010-01-01 00:00:00_2011-01-01 ...       307094   \n",
      "\n",
      "                                             test_id  test_n_obs  \n",
      "0  dropout_binary_2010-01-01 00:00:00_2011-01-01 ...      307094  \n",
      "1  dropout_binary_2011-01-01 00:00:00_2012-01-01 ...      320042  \n",
      "0.0    2162050\n",
      "1.0      25784\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_test_matrices = get_train_test_matrix_pairs(conn, experiment_hash, model_group_id)\n",
    "entity_to_attrib = get_entity_to_attrib_simple(conn, race_query, sa_column, anchor_sa_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    727281\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../fair-classification/disparate_mistreatment/synthetic_data_demo')\n",
    "sys.path.insert(0, '../fair-classification/fair_classification/')\n",
    "from generate_synthetic_data import *\n",
    "import utils as ut\n",
    "import funcs_disp_mist as fdm\n",
    "import plot_syn_boundaries as psb\n",
    "import pandas as pd\n",
    "import cvxpy\n",
    "import yaml\n",
    "from triage import create_engine\n",
    "from triage.component.catwalk.estimators.classifiers import ScaledLogisticRegression\n",
    "import time\n",
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_top_k(distances, k):\n",
    "    sort_df = pd.DataFrame({'dist': distances})\n",
    "    sort_df.sort_values('dist', ascending=False, inplace=True)\n",
    "    sort_df['pred_label'] = -1\n",
    "    sort_df['orig_idx'] = sort_df.index\n",
    "    sort_df.reset_index(inplace=True)\n",
    "    i = k-1\n",
    "    sort_df.loc[:i,'pred_label'] = 1\n",
    "    sort_df.sort_values('orig_idx', inplace=True)\n",
    "    \n",
    "    return sort_df['pred_label'].values\n",
    "\n",
    "\n",
    "def read_config_file(config_file):\n",
    "    config = None\n",
    "    try:\n",
    "        with open (config_file, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error reading the config file')\n",
    "    return config\n",
    "\n",
    "def connect(cred_folder):\n",
    "    cred_file = os.path.join(cred_folder, 'joco_db_profile.yaml')\n",
    "    db = read_config_file(cred_file)\n",
    "\n",
    "    sql_engine = create_engine(\n",
    "        'postgresql+psycopg2://%s:%s@%s:%i/%s'%(\n",
    "            db['user'],\n",
    "            db['pass'],\n",
    "            db['host'],\n",
    "            db['port'],\n",
    "            db['db']\n",
    "        )\n",
    "    )\n",
    "    return sql_engine\n",
    "\n",
    "def load_matrix(FILE_PATH, matrix_id, entity_to_attrib, demo_col, label_col):\n",
    "    df = pd.read_csv('%s/%s.csv.gz' % (FILE_PATH, matrix_id), compression='gzip')\n",
    "\n",
    "    entity_col = []\n",
    "    entities = df['entity_id'].values\n",
    "    \n",
    "    for i in range(len(entities)):\n",
    "        try:\n",
    "            attr = entity_to_attrib[int(entities[i])]\n",
    "            entity_col.append(attr)\n",
    "        except KeyError as e:\n",
    "            entity_col.append(\"MISSING\")\n",
    "\n",
    "    df[demo_col] = entity_col\n",
    "    df = df[df[demo_col]!='MISSING']\n",
    "    df[label_col] = 2*(df[label_col] - 0.5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_matrix_from_df(df, demo_col, label_col, cols_0):\n",
    "    exclude_cols = ['entity_id', 'as_of_date', label_col, demo_col]\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if c in cols_0:\n",
    "            exclude_cols.append(c)\n",
    "\n",
    "    x = df[[c for c in df.columns if c not in exclude_cols]].values\n",
    "    y = df[label_col].values\n",
    "    x_control = {demo_col:df[demo_col].values}\n",
    "\n",
    "    return x, y, x_control\n",
    "\n",
    "\n",
    "def get_train_test_matrix_pairs(engine, experiment_hash, model_group_id):\n",
    "    '''\n",
    "    args:\n",
    "        engine: PSQL Connection Engine\n",
    "        experiment_hash: use the relevant experiment_hash attributing to the train/test matrices you want to obtain\n",
    "        model_group_id: it might be better to use a model_group_id corresponding to \n",
    "        the dummy classifier for which train/test matrices might have been created; otherwise any relevant model_group_id\n",
    "        will do.\n",
    "    '''\n",
    "\n",
    "    query = \"\"\"\n",
    "    with rel_models as\n",
    "    (\n",
    "        select model_id, model_hash \n",
    "        from model_metadata.models\n",
    "        where \n",
    "            built_by_experiment = '%s'\n",
    "            and model_group_id = %s\n",
    "    ),\n",
    "    train_matrices as\n",
    "    (\n",
    "        select model_id, matrix_uuid from\n",
    "        train_results.prediction_metadata\n",
    "    ),\n",
    "    test_matrices as \n",
    "    (\n",
    "        select model_id, matrix_uuid from \n",
    "        test_results.prediction_metadata\n",
    "    ),\n",
    "    matrix_info as \n",
    "    (\n",
    "        select matrix_id, matrix_uuid,\n",
    "        matrix_type, num_observations\n",
    "        from model_metadata.matrices\n",
    "    )\n",
    "    select \n",
    "        rel_models.model_id, \n",
    "        train_matrices.matrix_uuid as train_matrix_id,\n",
    "        test_matrices.matrix_uuid as test_matrix_id,\n",
    "        m1.matrix_id as train_id, \n",
    "        m1.num_observations as train_n_obs,\n",
    "        m2.matrix_id as test_id,\n",
    "        m2.num_observations as test_n_obs\n",
    "    from \n",
    "        rel_models, \n",
    "        train_matrices, test_matrices,\n",
    "        matrix_info m1, matrix_info m2\n",
    "    where\n",
    "        rel_models.model_id = train_matrices.model_id \n",
    "    and\n",
    "        rel_models.model_id = test_matrices.model_id\n",
    "    and \n",
    "        m1.matrix_uuid = train_matrices.matrix_uuid\n",
    "    and \n",
    "        m2.matrix_uuid = test_matrices.matrix_uuid;\n",
    "    \"\"\"%(str(experiment_hash), str(model_group_id))\n",
    "\n",
    "    #print(query)\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(df.head(2))\n",
    "    \n",
    "    vals = df.values\n",
    "    train_test_matrices = []\n",
    "    \n",
    "    for v in vals:\n",
    "        train_test_matrices.append([(v[1], v[2])])\n",
    "\n",
    "    return train_test_matrices\n",
    "\n",
    "def get_entity_to_attrib_simple(conn, race_query, sa_column, anchor_sa_value):\n",
    "    entity_to_attrib = {}\n",
    "    df = pd.read_sql(race_query, conn)\n",
    "    \n",
    "    entity_ids = df['entity_id'].values\n",
    "    sa_vals = df[sa_column].values\n",
    "    \n",
    "    attrib_info = {}\n",
    "\n",
    "    for i in range(len(entity_ids)):\n",
    "        if(sa_vals[i]==anchor_sa_value):\n",
    "            sa_vals[i] = 1\n",
    "        else:\n",
    "            sa_vals[i] = 0.0\n",
    "        attrib_info[int(entity_ids[i])] = sa_vals[i]\n",
    "        \n",
    "    print(pd.value_counts(sa_vals))\n",
    "    return attrib_info\n",
    "\n",
    "def calc_prec(pred_label, actual_label):\n",
    "    label_pos = sum((pred_label == 1).astype(int))\n",
    "    true_pos = sum(np.logical_and(pred_label == actual_label, pred_label == 1).astype(int))\n",
    "    return float(true_pos/label_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect('../../config')\n",
    "experiment_hash='7bfb397f00416f59fb409fde51e6b6fa'\n",
    "race_query='select entity_id, race from hemank_bias_alternatives.currmatch_entity_demos'\n",
    "label_col='booking_view_warr_bw_1y'\n",
    "demo_col='race'\n",
    "model_group_id=55\n",
    "sa_column='race'\n",
    "anchor_sa_value='W'\n",
    "file_path='/mnt/data/experiment_data/joco/joco_original/matrices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_matrices = get_train_test_matrix_pairs(conn, experiment_hash, model_group_id)\n",
    "entity_to_attrib = get_entity_to_attrib_simple(conn, race_query, sa_column, anchor_sa_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "train_matrix_uuid = train_test_matrices[i][0][0]\n",
    "test_matrix_uuid = train_test_matrices[i][0][1]\n",
    "\n",
    "print(train_matrix_uuid)\n",
    "print(test_matrix_uuid)\n",
    "\n",
    "df_train = load_matrix(file_path, train_matrix_uuid, entity_to_attrib, demo_col, label_col)\n",
    "df_train['as_of_date'] = pd.to_datetime(df_train['as_of_date'])\n",
    "train_as_of_dates = df_train['as_of_date'].values\n",
    "train_entity_ids = df_train['entity_id'].values\n",
    "\n",
    "print(len(df_train.columns))\n",
    "df_test = load_matrix(file_path, test_matrix_uuid, entity_to_attrib, demo_col, label_col)\n",
    "df_test['as_of_date'] = pd.to_datetime(df_test['as_of_date'])\n",
    "test_as_of_dates = df_test['as_of_date'].values\n",
    "test_entity_ids = df_test['entity_id'].values\n",
    "\n",
    "print(len(df_test.columns))\n",
    "\n",
    "df_train['intercept'] = 1\n",
    "df_test['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_exclude_cols = ['entity_id', 'as_of_date', label_col]\n",
    "\n",
    "x_temp = df_train[[c for c in df_train.columns if c not in old_exclude_cols]].values\n",
    "y_temp = df_train[label_col].values\n",
    "\n",
    "dsapp_lr = ScaledLogisticRegression(penalty='l1', C=0.1)\n",
    "dsapp_lr.fit(x_temp, y_temp)\n",
    "\n",
    "print(dsapp_lr.coef_)\n",
    "print(dsapp_lr.intercept_)\n",
    "\n",
    "all_columns = [c for c in df_train.columns if c not in old_exclude_cols]\n",
    "keep_cols = []\n",
    "\n",
    "for i, col in enumerate(all_columns):\n",
    "    if dsapp_lr.coef_[0][i]!=0:\n",
    "        keep_cols.append(col)\n",
    "        \n",
    "keep_cols = keep_cols + ['intercept']\n",
    "\n",
    "x_train = df_train[[c for c in df_train.columns if c in keep_cols]].values\n",
    "y_train = df_train[label_col].values\n",
    "x_control_train = {demo_col: df_train[demo_col].values}\n",
    "\n",
    "x_test = df_test[[c for c in df_test.columns if c in keep_cols]].values\n",
    "y_test = df_test[label_col].values\n",
    "x_control_test = {demo_col: df_test[demo_col].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_control_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(df_train.shape)\n",
    "print(len(keep_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "x_control = x_control_train\n",
    "\n",
    "max_iters = 150\n",
    "max_iters_dccp = 75\n",
    "        \n",
    "num_points, num_features = x.shape\n",
    "w = cvxpy.Variable(num_features)\n",
    "\n",
    "np.random.seed(112233)\n",
    "w.value = np.random.rand(x.shape[1])\n",
    "\n",
    "constraints = []\n",
    "loss = cvxpy.sum(  cvxpy.logistic( cvxpy.multiply(-y, x*w) )  ) / num_points\n",
    "prob = cvxpy.Problem(cvxpy.Minimize(loss), constraints)\n",
    "\n",
    "tau =  float(0.005)\n",
    "mu = float(1.2)\n",
    "\n",
    "loss_function = \"logreg\" # perform the experiments with logistic regression\n",
    "EPS = float(1e-4)\n",
    "\n",
    "prob.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, solver=cvxpy.ECOS, verbose=True, feastol=EPS, abstol=EPS, reltol=EPS, feastol_inacc=EPS, abstol_inacc=EPS, reltol_inacc=EPS,max_iters=max_iters, max_iter=max_iters_dccp)\n",
    "\n",
    "ret_w = np.array(w.value).flatten()\n",
    "sensitive_attrs = list(x_control_train.keys())\n",
    "\n",
    "print(\"INPUT TRAIN:\"+str(pd.value_counts(y_train)))\n",
    "print(\"INPUT TEST:\"+str(pd.value_counts(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = fdm.get_clf_stats(ret_w, x_train, y_train, x_control, x_test, y_test, x_control_test, list(sensitive_attrs))\n",
    "s_attr = sensitive_attrs[0]\n",
    "\n",
    "print(s_attr)\n",
    "print(pd.value_counts(x_control_test[s_attr]))\n",
    "distances_boundary_test = fdm.get_distance_boundary(ret_w, x_test, x_control_test[s_attr])\n",
    "print(pd.value_counts(y_test))\n",
    "print(pd.value_counts(distances_boundary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distances_boundary_test[y_test==1], bins=600, color='blue')\n",
    "plt.hist(distances_boundary_test[y_test==-1], bins=600, color='red')\n",
    "plt.xlim([-100,100])\n",
    "plt.xlim([-100,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "all_class_labels_assigned_test = label_top_k(distances_boundary_test, k)\n",
    "prec_k = calc_prec(all_class_labels_assigned_test, y_test)\n",
    "print('prec@%s_abs: %.5f' % (k, prec_k))\n",
    "\n",
    "s_attr_to_fp_fn_test = fdm.get_fpr_fnr_sensitive_features(y_test, all_class_labels_assigned_test, x_control_test, sensitive_attrs, False)\n",
    "        \n",
    "    \n",
    "for s_attr in s_attr_to_fp_fn_test.keys():\n",
    "    for s_val in s_attr_to_fp_fn_test[s_attr].keys():\n",
    "        s_attr_to_fp_fn_test[s_attr][s_val]['recall'] = 1.000-s_attr_to_fp_fn_test[s_attr][s_val]['fnr']\n",
    "\n",
    "recall_nonwhite = s_attr_to_fp_fn_test['race'][0]['recall']\n",
    "recall_white = s_attr_to_fp_fn_test['race'][1]['recall']\n",
    "\n",
    "\n",
    "print('recall non-white: %.6f' % recall_nonwhite)\n",
    "print('recall white: %.6f' % recall_white)\n",
    "print('recall ratio: %.6f' % float(recall_white/recall_nonwhite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_exclude_cols = ['entity_id', 'as_of_date', label_col]\n",
    "\n",
    "x_temp = df_train[[c for c in df_train.columns if c not in old_exclude_cols]].values\n",
    "y_temp = df_train[label_col].values\n",
    "\n",
    "x_train = df_train[[c for c in df_train.columns if c not in ['entity_id', 'as_of_date', label_col, demo_col]]].values\n",
    "y_train = df_train[label_col].values\n",
    "x_control_train = {demo_col: df_train[demo_col].values}\n",
    "\n",
    "x_test = df_test[[c for c in df_train.columns if c not in ['entity_id', 'as_of_date', label_col, demo_col]]].values\n",
    "y_test = df_test[label_col].values\n",
    "x_control_test = {demo_col: df_test[demo_col].values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "x_control = x_control_train\n",
    "\n",
    "max_iters = 150\n",
    "max_iters_dccp = 75\n",
    "        \n",
    "num_points, num_features = x.shape\n",
    "w = cvxpy.Variable(num_features)\n",
    "\n",
    "np.random.seed(112233)\n",
    "w.value = np.random.rand(x.shape[1])\n",
    "\n",
    "constraints = []\n",
    "loss = cvxpy.sum(  cvxpy.logistic( cvxpy.multiply(-y, x*w) )  ) / num_points\n",
    "prob = cvxpy.Problem(cvxpy.Minimize(loss), constraints)\n",
    "\n",
    "tau =  float(0.005)\n",
    "mu = float(1.2)\n",
    "\n",
    "loss_function = \"logreg\" # perform the experiments with logistic regression\n",
    "EPS = float(1e-4)\n",
    "\n",
    "prob.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, solver=cvxpy.ECOS, verbose=True, feastol=EPS, abstol=EPS, reltol=EPS, feastol_inacc=EPS, abstol_inacc=EPS, reltol_inacc=EPS,max_iters=max_iters, max_iter=max_iters_dccp)\n",
    "\n",
    "ret_w = np.array(w.value).flatten()\n",
    "sensitive_attrs = list(x_control_train.keys())\n",
    "\n",
    "print(\"INPUT TRAIN:\"+str(pd.value_counts(y_train)))\n",
    "print(\"INPUT TEST:\"+str(pd.value_counts(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = fdm.get_clf_stats(ret_w, x_train, y_train, x_control, x_test, y_test, x_control_test, list(sensitive_attrs))\n",
    "s_attr = sensitive_attrs[0]\n",
    "\n",
    "print(s_attr)\n",
    "print(pd.value_counts(x_control_test[s_attr]))\n",
    "distances_boundary_test = fdm.get_distance_boundary(ret_w, x_test, x_control_test[s_attr])\n",
    "print(pd.value_counts(y_test))\n",
    "print(pd.value_counts(distances_boundary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "all_class_labels_assigned_test = label_top_k(distances_boundary_test, k)\n",
    "prec_k = calc_prec(all_class_labels_assigned_test, y_test)\n",
    "print('prec@%s_abs: %.5f' % (k, prec_k))\n",
    "\n",
    "s_attr_to_fp_fn_test = fdm.get_fpr_fnr_sensitive_features(y_test, all_class_labels_assigned_test, x_control_test, sensitive_attrs, False)\n",
    "        \n",
    "    \n",
    "for s_attr in s_attr_to_fp_fn_test.keys():\n",
    "    for s_val in s_attr_to_fp_fn_test[s_attr].keys():\n",
    "        s_attr_to_fp_fn_test[s_attr][s_val]['recall'] = 1.000-s_attr_to_fp_fn_test[s_attr][s_val]['fnr']\n",
    "\n",
    "recall_nonwhite = s_attr_to_fp_fn_test['race'][0]['recall']\n",
    "recall_white = s_attr_to_fp_fn_test['race'][1]['recall']\n",
    "\n",
    "\n",
    "print('recall non-white: %.6f' % recall_nonwhite)\n",
    "print('recall white: %.6f' % recall_white)\n",
    "print('recall ratio: %.6f' % float(recall_white/recall_nonwhite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select entity_id, race, race_3way from hemank_bias_alternatives.currmatch_entity_demos\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "values = df.values\n",
    "entity_to_race = {}\n",
    "for i in range(len(values)):\n",
    "    entity_to_race[values[i][0]] = values[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select entity_id, race, race_3way from kit_bias_class_test.currmatch_entity_demos\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "values = df.values\n",
    "entity_to_race2 = {}\n",
    "for i in range(len(values)):\n",
    "    entity_to_race2[values[i][0]] = values[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = pd.read_csv('/mnt/data/experiment_data/joco/joco_original/matrices/0ea5e6dd955ce57a0de8ee9beca0996f.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_rel = m1[['entity_id', 'booking_view_warr_bw_1y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve(x):\n",
    "    try:\n",
    "        return entity_to_race[x]\n",
    "    except KeyError as e:\n",
    "        return None\n",
    "\n",
    "def resolve2(x):\n",
    "    try:\n",
    "        return entity_to_race2[x]\n",
    "    except KeyError as e:\n",
    "        return None\n",
    "\n",
    "m1_rel['race'] = m1_rel['entity_id'].apply(lambda x: resolve(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(m1_rel['race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(m1_rel[m1_rel['race']=='W']['booking_view_warr_bw_1y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(m1_rel[m1_rel['race']!='W']['booking_view_warr_bw_1y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_2 = pd.read_csv('/mnt/data/experiment_data/joco2/under_original_same_nop_original/matrices/0ea5e6dd955ce57a0de8ee9beca0996f.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m12_rel = m1_2[['entity_id', 'booking_view_warr_bw_1y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m12_rel['race'] = m12_rel['entity_id'].apply(lambda x: resolve(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = pd.read_csv('/mnt/data/experiment_data/joco2/joco_original/matrices/0ea5e6dd955ce57a0de8ee9beca0996f.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_rel = m2[['entity_id', 'booking_view_warr_bw_1y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_rel['race'] = m2_rel['entity_id'].apply(lambda x: resolve(x))\n",
    "m2_rel['race2'] = m2_rel['entity_id'].apply(lambda x:resolve2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(m2_rel[m2_rel['race']=='W']['booking_view_warr_bw_1y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(m2_rel[m2_rel['race2']=='W']['booking_view_warr_bw_1y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsapp_lr.coef_[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['schema'] = 'joco_bias_under_orig_50_orig'\n",
    "sql_us = query.render(**params)\n",
    "ts_df_u_v2c = pd.read_sql(sql_us, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_df = pd.read_csv('/mnt/data/experiment_data/joco2/under_original_50_original/matrices/cf42d6c89aa0e48384c15088ebc7b289.csv.gz', \n",
    "                        compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_attribute = {}\n",
    "eab = pd.read_sql(\"\"\"select entity_id, race_2way from hemank_bias_alternatives.currmatch_entity_demos\n",
    "\"\"\", conn)\n",
    "\n",
    "eab_eids = eab['entity_id'].values\n",
    "eab_r2w = eab['race_2way'].values\n",
    "\n",
    "for i in range(len(eab_eids)):\n",
    "    entity_to_attribute[eab_eids[i]] = eab_r2w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_ids = matrix_df['entity_id'].values\n",
    "white = []\n",
    "nw = []\n",
    "\n",
    "for eid in entity_ids:\n",
    "    try:\n",
    "        r = entity_to_attribute[int(eid/1e+7)]\n",
    "    except KeyError as e:\n",
    "        r = 'Missing'\n",
    "        \n",
    "    if r=='White':\n",
    "        white.append(eid)\n",
    "    if r=='NonWhite':\n",
    "        nw.append(eid)\n",
    "        \n",
    "print(len(white), len(nw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.read_sql(\"\"\"\n",
    "select model_id, model_group_id, base_value, adj_value, base_recall_white_to_nonwhite, adj_recall_white_to_nonwhite from\n",
    "joco_bias_under_orig_50_orig.model_adjustment_results_race_2way\n",
    "where train_end_time='2014-04-01'::date\n",
    "\"\"\", conn)\n",
    "df_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_query = \"\"\"\n",
    "truncate table test_results.zafar_predictions;\n",
    "\"\"\"\n",
    "conn.execute(truncate_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
