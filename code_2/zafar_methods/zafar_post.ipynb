{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemank/.pyenv/versions/3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:15: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display \n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline\n",
    "\n",
    "def connect(poolclass=sqlalchemy.pool.QueuePool):\n",
    "    with open(os.path.join(os.path.join('../..','config'), 'joco_db_profile.yaml')) as fd:\n",
    "        config = yaml.load(fd)\n",
    "        dburl = sqlalchemy.engine.url.URL(\n",
    "            \"postgres\",\n",
    "            host=config[\"host\"],\n",
    "            username=config[\"user\"],\n",
    "            database=config[\"db\"],\n",
    "            password=config[\"pass\"],\n",
    "            port=config[\"port\"],\n",
    "        )\n",
    "        return sqlalchemy.create_engine(dburl, poolclass=poolclass)\n",
    "\n",
    "    \n",
    "conn = connect()\n",
    "\n",
    "import SAVE_RecallAdjuster as sra\n",
    "from importlib import reload\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2014-04-01', '2014-04-01'), ('2014-04-01', '2015-04-01'), ('2014-08-01', '2014-08-01'), ('2014-08-01', '2015-08-01'), ('2014-12-01', '2014-12-01'), ('2014-12-01', '2015-12-01'), ('2015-04-01', '2015-04-01'), ('2015-04-01', '2016-04-01'), ('2015-08-01', '2015-08-01'), ('2015-08-01', '2016-08-01'), ('2015-12-01', '2015-12-01'), ('2015-12-01', '2016-12-01'), ('2016-04-01', '2016-04-01'), ('2016-04-01', '2017-04-01'), ('2016-08-01', '2016-08-01'), ('2016-08-01', '2017-08-01'), ('2016-12-01', '2016-12-01'), ('2016-12-01', '2017-12-01'), ('2017-04-01', '2017-04-01'), ('2017-04-01', '2018-04-01')]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "base = datetime.datetime.strptime('2018-04-01', '%Y-%m-%d')\n",
    "date_pairs = []\n",
    "for x in range(9,-1,-1):\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=4*x) - relativedelta(years=1)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=4*x) - relativedelta(years=1)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "    date_pairs.append(\n",
    "        (\n",
    "        (base - relativedelta(months=4*x) - relativedelta(years=1)).strftime('%Y-%m-%d'),\n",
    "        (base - relativedelta(months=4*x)).strftime('%Y-%m-%d')\n",
    "        )\n",
    "    )\n",
    "\n",
    "import seaborn as sns\n",
    "print(date_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not executing Entity Demos -- picking up from previously built schema.\n",
      "Done creating entities\n",
      "Setting demo values. Not using pre-set values\n",
      "Not running subsampling OR bootstrap based models\n",
      "Running Recall Adjustment\n",
      "Time Taken=3.093769073486328\n"
     ]
    }
   ],
   "source": [
    "reload(sra)\n",
    "start_time = time.time()\n",
    "myRA_us_frac_3 = sra.RecallAdjuster(\n",
    "        engine=conn,\n",
    "        pg_role='hemank',\n",
    "        schema='hemank_bias_zafar',\n",
    "        experiment_hashes='None',\n",
    "        date_pairs=date_pairs,\n",
    "        list_sizes=[500],\n",
    "        entity_demos='hemank_bias_alternatives.currmatch_entity_demos',\n",
    "        demo_col='race_2way',\n",
    "        start_model_id = 70,\n",
    "        end_model_id = 71\n",
    ")\n",
    "print(\"Time Taken=\"+str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp - Running Zafar ONLY to see their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../fair-classification/disparate_mistreatment/synthetic_data_demo')\n",
    "sys.path.insert(0, '../fair-classification/fair_classification/')\n",
    "from generate_synthetic_data import *\n",
    "import utils as ut\n",
    "import funcs_disp_mist as fdm\n",
    "import plot_syn_boundaries as psb\n",
    "import pandas as pd\n",
    "import cvxpy\n",
    "import yaml\n",
    "from triage import create_engine\n",
    "from triage.component.catwalk.estimators.classifiers import ScaledLogisticRegression\n",
    "import time\n",
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_top_k(distances, k):\n",
    "    sort_df = pd.DataFrame({'dist': distances})\n",
    "    sort_df.sort_values('dist', ascending=False, inplace=True)\n",
    "    sort_df['pred_label'] = -1\n",
    "    sort_df['orig_idx'] = sort_df.index\n",
    "    sort_df.reset_index(inplace=True)\n",
    "    i = k-1\n",
    "    sort_df.loc[:i,'pred_label'] = 1\n",
    "    sort_df.sort_values('orig_idx', inplace=True)\n",
    "    \n",
    "    return sort_df['pred_label'].values\n",
    "\n",
    "\n",
    "def read_config_file(config_file):\n",
    "    config = None\n",
    "    try:\n",
    "        with open (config_file, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error reading the config file')\n",
    "    return config\n",
    "\n",
    "def connect(cred_folder):\n",
    "    cred_file = os.path.join(cred_folder, 'joco_db_profile.yaml')\n",
    "    db = read_config_file(cred_file)\n",
    "\n",
    "    sql_engine = create_engine(\n",
    "        'postgresql+psycopg2://%s:%s@%s:%i/%s'%(\n",
    "            db['user'],\n",
    "            db['pass'],\n",
    "            db['host'],\n",
    "            db['port'],\n",
    "            db['db']\n",
    "        )\n",
    "    )\n",
    "    return sql_engine\n",
    "\n",
    "def load_matrix(FILE_PATH, matrix_id, entity_to_attrib, demo_col, label_col):\n",
    "    df = pd.read_csv('%s/%s.csv.gz' % (FILE_PATH, matrix_id), compression='gzip')\n",
    "\n",
    "    entity_col = []\n",
    "    entities = df['entity_id'].values\n",
    "    \n",
    "    for i in range(len(entities)):\n",
    "        try:\n",
    "            attr = entity_to_attrib[int(entities[i])]\n",
    "            entity_col.append(attr)\n",
    "        except KeyError as e:\n",
    "            entity_col.append(\"MISSING\")\n",
    "\n",
    "    df[demo_col] = entity_col\n",
    "    df = df[df[demo_col]!='MISSING']\n",
    "    df[label_col] = 2*(df[label_col] - 0.5)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_matrix_from_df(df, demo_col, label_col, cols_0):\n",
    "    exclude_cols = ['entity_id', 'as_of_date', label_col, demo_col]\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if c in cols_0:\n",
    "            exclude_cols.append(c)\n",
    "\n",
    "    x = df[[c for c in df.columns if c not in exclude_cols]].values\n",
    "    y = df[label_col].values\n",
    "    x_control = {demo_col:df[demo_col].values}\n",
    "\n",
    "    return x, y, x_control\n",
    "\n",
    "\n",
    "def get_train_test_matrix_pairs(engine, experiment_hash, model_group_id):\n",
    "    '''\n",
    "    args:\n",
    "        engine: PSQL Connection Engine\n",
    "        experiment_hash: use the relevant experiment_hash attributing to the train/test matrices you want to obtain\n",
    "        model_group_id: it might be better to use a model_group_id corresponding to \n",
    "        the dummy classifier for which train/test matrices might have been created; otherwise any relevant model_group_id\n",
    "        will do.\n",
    "    '''\n",
    "\n",
    "    query = \"\"\"\n",
    "    with rel_models as\n",
    "    (\n",
    "        select model_id, model_hash \n",
    "        from model_metadata.models\n",
    "        where \n",
    "            built_by_experiment = '%s'\n",
    "            and model_group_id = %s\n",
    "    ),\n",
    "    train_matrices as\n",
    "    (\n",
    "        select model_id, matrix_uuid from\n",
    "        train_results.prediction_metadata\n",
    "    ),\n",
    "    test_matrices as \n",
    "    (\n",
    "        select model_id, matrix_uuid from \n",
    "        test_results.prediction_metadata\n",
    "    ),\n",
    "    matrix_info as \n",
    "    (\n",
    "        select matrix_id, matrix_uuid,\n",
    "        matrix_type, num_observations\n",
    "        from model_metadata.matrices\n",
    "    )\n",
    "    select \n",
    "        rel_models.model_id, \n",
    "        train_matrices.matrix_uuid as train_matrix_id,\n",
    "        test_matrices.matrix_uuid as test_matrix_id,\n",
    "        m1.matrix_id as train_id, \n",
    "        m1.num_observations as train_n_obs,\n",
    "        m2.matrix_id as test_id,\n",
    "        m2.num_observations as test_n_obs\n",
    "    from \n",
    "        rel_models, \n",
    "        train_matrices, test_matrices,\n",
    "        matrix_info m1, matrix_info m2\n",
    "    where\n",
    "        rel_models.model_id = train_matrices.model_id \n",
    "    and\n",
    "        rel_models.model_id = test_matrices.model_id\n",
    "    and \n",
    "        m1.matrix_uuid = train_matrices.matrix_uuid\n",
    "    and \n",
    "        m2.matrix_uuid = test_matrices.matrix_uuid;\n",
    "    \"\"\"%(str(experiment_hash), str(model_group_id))\n",
    "\n",
    "    #print(query)\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(df.head(2))\n",
    "    \n",
    "    vals = df.values\n",
    "    train_test_matrices = []\n",
    "    \n",
    "    for v in vals:\n",
    "        train_test_matrices.append([(v[1], v[2])])\n",
    "\n",
    "    return train_test_matrices\n",
    "\n",
    "def get_entity_to_attrib_simple(conn, race_query, sa_column, anchor_sa_value):\n",
    "    entity_to_attrib = {}\n",
    "    df = pd.read_sql(race_query, conn)\n",
    "    \n",
    "    entity_ids = df['entity_id'].values\n",
    "    sa_vals = df[sa_column].values\n",
    "    \n",
    "    attrib_info = {}\n",
    "\n",
    "    for i in range(len(entity_ids)):\n",
    "        if(sa_vals[i]==anchor_sa_value):\n",
    "            sa_vals[i] = 1\n",
    "        else:\n",
    "            sa_vals[i] = 0.0\n",
    "        attrib_info[int(entity_ids[i])] = sa_vals[i]\n",
    "        \n",
    "    print(pd.value_counts(sa_vals))\n",
    "    return attrib_info\n",
    "\n",
    "def calc_prec(pred_label, actual_label):\n",
    "    label_pos = sum((pred_label == 1).astype(int))\n",
    "    true_pos = sum(np.logical_and(pred_label == actual_label, pred_label == 1).astype(int))\n",
    "    return float(true_pos/label_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect('../../config')\n",
    "experiment_hash='53918b13ea1115d6b5d2b5b16ef4e2f4'\n",
    "race_query='select entity_id, race from hemank_bias_alternatives.currmatch_entity_demos'\n",
    "label_col='booking_view_warr_bw_1y'\n",
    "demo_col='race'\n",
    "model_group_id=55\n",
    "sa_column='race'\n",
    "anchor_sa_value='W'\n",
    "file_path='/mnt/data/experiment_data/peeps/joco_original/matrices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model_id                   train_matrix_id  \\\n",
      "0      1182  cf42d6c89aa0e48384c15088ebc7b289   \n",
      "1      1233  df07e7bc5f1a151d38ff680f16410781   \n",
      "\n",
      "                     test_matrix_id  \\\n",
      "0  44971539154916ae818762ba67595d33   \n",
      "1  181d2f2f779c9215b1b78583f2aeae70   \n",
      "\n",
      "                                            train_id  train_n_obs  \\\n",
      "0  booking_view_warr_bw_1y_binary_2013-04-01 00:0...        28658   \n",
      "1  booking_view_warr_bw_1y_binary_2013-08-01 00:0...        28335   \n",
      "\n",
      "                                             test_id  test_n_obs  \n",
      "0  booking_view_warr_bw_1y_binary_2014-04-01 00:0...       27908  \n",
      "1  booking_view_warr_bw_1y_binary_2014-08-01 00:0...       27534  \n",
      "1.0    266050\n",
      "0.0    171473\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_test_matrices = get_train_test_matrix_pairs(conn, experiment_hash, model_group_id)\n",
    "entity_to_attrib = get_entity_to_attrib_simple(conn, race_query, sa_column, anchor_sa_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cf42d6c89aa0e48384c15088ebc7b289\n",
      "44971539154916ae818762ba67595d33\n",
      "3469\n",
      "3469\n",
      "==========\n",
      "100000080\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "100000080\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "train_matrix_uuid = train_test_matrices[i][0][0]\n",
    "test_matrix_uuid = train_test_matrices[i][0][1]\n",
    "\n",
    "print(train_matrix_uuid)\n",
    "print(test_matrix_uuid)\n",
    "\n",
    "df_train = load_matrix(file_path, train_matrix_uuid, entity_to_attrib, demo_col, label_col)\n",
    "df_train['as_of_date'] = pd.to_datetime(df_train['as_of_date'])\n",
    "train_as_of_dates = df_train['as_of_date'].values\n",
    "train_entity_ids = df_train['entity_id'].values\n",
    "\n",
    "\n",
    "\n",
    "print(len(df_train.columns))\n",
    "\n",
    "df_test = load_matrix(file_path, test_matrix_uuid, entity_to_attrib, demo_col, label_col)\n",
    "df_test['as_of_date'] = pd.to_datetime(df_test['as_of_date'])\n",
    "test_as_of_dates = df_test['as_of_date'].values\n",
    "test_entity_ids = df_test['entity_id'].values\n",
    "\n",
    "print(len(df_test.columns))\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(train_entity_ids[0])\n",
    "print(x_train[0])\n",
    "print(test_entity_ids[0])\n",
    "print(x_test[0])\n",
    "print(\"=\"*10)\n",
    "        \n",
    "x_train, y_train, x_control_train = get_matrix_from_df(df_train, demo_col, label_col, [])\n",
    "x_test, y_test, x_control_test = get_matrix_from_df(df_test, demo_col, label_col, [])\n",
    "\n",
    "# Perform Scaled Logistic Regression to include only relevant features.\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "dsapp_lr = ScaledLogisticRegression()\n",
    "dsapp_lr.fit(x_train, y_train)\n",
    "\n",
    "print(dsapp_lr.coef_)\n",
    "print(dsapp_lr.intercept_)\n",
    "\n",
    "all_cols = [c for c in df_train.columns if c not in ['as_of_date', 'entity_id', demo_col, label_col]]\n",
    "exclude_cols = []\n",
    "\n",
    "for i, col in enumerate(all_cols):\n",
    "    if dsapp_lr.coef_[0][i] == 0:\n",
    "        exclude_cols.append(col)\n",
    "#exclude_cols = keep_cols + ['intercept']\n",
    "\n",
    "df_train['intercept'] = 1\n",
    "df_test['intercept'] = 1\n",
    "\n",
    "print(len(exclude_cols))\n",
    "print(len(all_cols) - len(exclude_cols))\n",
    "\n",
    "x_train, y_train, x_control_train = get_matrix_from_df(df_train, demo_col, label_col, exclude_cols)\n",
    "x_test, y_test, x_control_test = get_matrix_from_df(df_test, demo_col, label_col, exclude_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28334, 2470) (27533, 2470) (28334,) (27533,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ECOS 2.0.7 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +0.000e+00  -3.679e+04  +2e+05  6e-01  2e+02  1e+00  1e+00    ---    ---    0  0  - |  -  - \n",
      " 1  +7.625e-01  -2.216e+04  +1e+05  2e-01  2e+02  6e-01  6e-01  0.5013  2e-01   2  1  2 |  4  3\n",
      " 2  +1.018e+00  -1.749e+04  +1e+05  1e-01  1e+02  5e-01  5e-01  0.5013  6e-01   1  1  2 |  8  3\n",
      " 3  +1.062e+00  -1.693e+04  +9e+04  1e-01  1e+02  5e-01  5e-01  0.9791  1e+00   1  1  1 | 20  0\n",
      " 4  +1.256e+00  -1.353e+04  +8e+04  9e-02  1e+02  4e-01  4e-01  0.4010  5e-01   1  2  1 |  7  4\n",
      " 5  +1.414e+00  -1.106e+04  +6e+04  7e-02  9e+01  3e-01  3e-01  0.9791  8e-01   1  2  2 | 12  0\n",
      " 6  +1.475e+00  -1.024e+04  +6e+04  6e-02  8e+01  3e-01  3e-01  0.9791  9e-01   1  1  2 | 16  0\n",
      " 7  +1.643e+00  -7.800e+03  +4e+04  4e-02  6e+01  2e-01  2e-01  0.4010  4e-01   2  2  2 |  6  4\n",
      " 8  +1.836e+00  -5.485e+03  +3e+04  3e-02  4e+01  2e-01  2e-01  0.5013  4e-01   2  1  1 |  6  3\n",
      " 9  +1.989e+00  -4.172e+03  +2e+04  2e-02  3e+01  1e-01  1e-01  0.4010  5e-01   2  1  2 |  6  4\n",
      "10  +2.167e+00  -2.933e+03  +2e+04  1e-02  2e+01  8e-02  9e-02  0.5013  5e-01   2  2  1 |  6  3\n",
      "11  +2.310e+00  -2.109e+03  +1e+04  1e-02  2e+01  6e-02  7e-02  0.4010  4e-01   2  1  1 |  5  4\n",
      "12  +2.451e+00  -1.267e+03  +9e+03  6e-03  1e+01  4e-02  4e-02  0.5013  3e-01   2  1  2 |  4  3\n",
      "13  +2.505e+00  -6.317e+02  +5e+03  3e-03  7e+00  2e-02  3e-02  0.5013  1e-01   2  2  1 |  2  3\n",
      "14  +2.420e+00  -2.095e+02  +2e+03  9e-04  3e+00  9e-03  1e-02  0.5988  3e-02   1  2  1 |  0  2\n",
      "15  +2.267e+00  -4.686e+01  +6e+02  2e-04  2e+00  3e-03  3e-03  0.7186  3e-02   1  1  1 |  0  1\n",
      "16  +2.112e+00  -1.021e+01  +2e+02  5e-05  2e+00  8e-04  8e-04  0.7430  3e-02   1  1  1 |  1  1\n",
      "17  +1.748e+00  -3.360e+00  +7e+01  2e-05  1e+00  3e-04  4e-04  0.6054  1e-01   1  1  1 |  2  2\n",
      "18  +1.198e+00  -5.703e-01  +3e+01  7e-06  8e-01  1e-04  2e-04  0.6266  8e-02   1  1  1 |  1  2\n",
      "19  +7.589e-01  -1.365e-01  +2e+01  4e-06  4e-01  7e-05  9e-05  0.5013  2e-01   1  1  1 |  2  3\n",
      "20  +4.693e-01  +1.097e-01  +8e+00  1e-06  2e-01  3e-05  4e-05  0.6266  1e-01   2  1  1 |  2  2\n",
      "21  +3.932e-01  +1.734e-01  +5e+00  9e-07  1e-01  2e-05  3e-05  0.5013  3e-01   1  1  1 |  4  3\n",
      "22  +3.624e-01  +1.987e-01  +4e+00  7e-07  8e-02  2e-06  2e-05  0.9791  7e-01   1  1  1 | 10  0\n",
      "23  +3.250e-01  +2.330e-01  +2e+00  4e-07  5e-02  1e-06  1e-05  0.5013  1e-01   1  1  1 |  3  3\n",
      "24  +3.049e-01  +2.492e-01  +1e+00  2e-07  3e-02  6e-07  6e-06  0.5013  2e-01   1  1  1 |  4  3\n",
      "25  +2.951e-01  +2.568e-01  +8e-01  2e-07  2e-02  3e-07  4e-06  0.6266  5e-01   2  1  1 |  7  2\n",
      "26  +2.940e-01  +2.575e-01  +8e-01  2e-07  2e-02  2e-07  4e-06  0.9791  9e-01   2  1  1 | 18  0\n",
      "27  +2.850e-01  +2.646e-01  +4e-01  8e-08  1e-02  9e-08  2e-06  0.5013  1e-01   2  1  1 |  3  3\n",
      "28  +2.818e-01  +2.670e-01  +3e-01  6e-08  8e-03  6e-08  2e-06  0.7833  7e-01   2  1  1 |  9  1\n",
      "29  +2.810e-01  +2.676e-01  +3e-01  6e-08  7e-03  6e-08  1e-06  0.9791  9e-01   2  1  1 | 15  0\n",
      "30  +2.780e-01  +2.699e-01  +2e-01  3e-08  4e-03  3e-08  9e-07  0.5013  2e-01   2  1  1 |  4  3\n",
      "31  +2.765e-01  +2.710e-01  +1e-01  2e-08  3e-03  2e-08  6e-07  0.7833  6e-01   2  1  1 |  8  1\n",
      "32  +2.759e-01  +2.714e-01  +1e-01  2e-08  2e-03  2e-08  5e-07  0.9791  8e-01   2  1  1 | 12  0\n",
      "33  +2.749e-01  +2.722e-01  +6e-02  1e-08  1e-03  1e-08  3e-07  0.5013  2e-01   1  1  1 |  4  3\n",
      "34  +2.742e-01  +2.726e-01  +3e-02  7e-09  9e-04  7e-09  2e-07  0.9791  6e-01   2  0  0 |  8  0\n",
      "35  +2.740e-01  +2.728e-01  +2e-02  5e-09  6e-04  5e-09  1e-07  0.9791  7e-01   1  1  1 | 10  0\n",
      "36  +2.737e-01  +2.730e-01  +2e-02  3e-09  4e-04  3e-09  8e-08  0.5013  3e-01   1  1  1 |  5  3\n",
      "37  +2.736e-01  +2.731e-01  +1e-02  2e-09  3e-04  2e-09  6e-08  0.9791  7e-01   2  0  0 | 10  0\n",
      "38  +2.735e-01  +2.732e-01  +7e-03  1e-09  2e-04  1e-09  4e-08  0.5013  2e-01   2  0  0 |  4  3\n",
      "39  +2.734e-01  +2.732e-01  +5e-03  9e-10  1e-04  9e-10  2e-08  0.9791  7e-01   2  0  0 |  9  0\n",
      "40  +2.734e-01  +2.733e-01  +3e-03  6e-10  8e-05  6e-10  2e-08  0.5013  3e-01   1  0  0 |  5  3\n",
      "41  +2.734e-01  +2.733e-01  +2e-03  4e-10  5e-05  4e-10  1e-08  0.9791  7e-01   2  0  0 | 10  0\n",
      "42  +2.733e-01  +2.733e-01  +1e-03  3e-10  3e-05  3e-10  7e-09  0.6266  4e-01   2  0  0 |  6  2\n",
      "43  +2.733e-01  +2.733e-01  +1e-03  3e-10  3e-05  3e-10  7e-09  0.9791  1e+00   2  0  0 | 20  0\n",
      "44  +2.733e-01  +2.733e-01  +9e-04  2e-10  2e-05  2e-10  4e-09  0.5013  3e-01   1  0  0 |  5  3\n",
      "45  +2.733e-01  +2.733e-01  +8e-04  1e-10  2e-05  1e-10  4e-09  0.9791  9e-01   2  0  0 | 14  0\n",
      "46  +2.733e-01  +2.733e-01  +6e-04  1e-10  2e-05  1e-10  3e-09  0.5013  6e-01   1  0  0 |  8  3\n",
      "47  +2.733e-01  +2.733e-01  +5e-04  1e-10  1e-05  1e-10  3e-09  0.9791  8e-01   1  0  0 | 13  0\n",
      "48  +2.733e-01  +2.733e-01  +4e-04  8e-11  1e-05  8e-11  2e-09  0.5013  6e-01   2  0  0 |  8  3\n",
      "49  +2.733e-01  +2.733e-01  +3e-04  7e-11  9e-06  7e-11  2e-09  0.9791  8e-01   1  0  0 | 13  0\n",
      "50  +2.733e-01  +2.733e-01  +3e-04  5e-11  7e-06  5e-11  1e-09  0.5013  6e-01   1  0  0 |  8  3\n",
      "51  +2.733e-01  +2.733e-01  +2e-04  5e-11  6e-06  5e-11  1e-09  0.9791  9e-01   1  0  0 | 14  0\n",
      "52  +2.733e-01  +2.733e-01  +2e-04  4e-11  5e-06  4e-11  9e-10  0.5013  6e-01   2  0  0 |  8  3\n",
      "53  +2.733e-01  +2.733e-01  +2e-04  3e-11  4e-06  3e-11  9e-10  0.9791  9e-01   1  0  1 | 18  0\n",
      "54  +2.733e-01  +2.733e-01  +1e-04  3e-11  4e-06  3e-11  7e-10  0.4010  5e-01   2  0  1 |  7  4\n",
      "55  +2.733e-01  +2.733e-01  +1e-04  2e-11  3e-06  2e-11  6e-10  0.9791  9e-01   2  0  0 | 15  0\n",
      "56  +2.733e-01  +2.733e-01  +1e-04  2e-11  3e-06  2e-11  5e-10  0.5013  7e-01   2  0  0 | 10  3\n",
      "57  +2.733e-01  +2.733e-01  +1e-04  2e-11  3e-06  2e-11  5e-10  0.9791  9e-01   1  0  0 | 16  0\n",
      "\n",
      "OPTIMAL (within feastol=2.6e-06, reltol=3.6e-04, abstol=9.9e-05).\n",
      "Runtime: 2354.092757 seconds.\n",
      "\n",
      "\n",
      "ECOS 2.0.7 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n",
      "\n",
      "It     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n",
      " 0  +0.000e+00  -3.679e+04  +2e+05  6e-01  2e+02  1e+00  1e+00    ---    ---    0  0  - |  -  - \n",
      " 1  +7.625e-01  -2.216e+04  +1e+05  2e-01  2e+02  6e-01  6e-01  0.5013  2e-01   2  1  2 |  4  3\n",
      " 2  +1.018e+00  -1.749e+04  +1e+05  1e-01  1e+02  5e-01  5e-01  0.5013  6e-01   1  1  2 |  8  3\n",
      " 3  +1.062e+00  -1.693e+04  +9e+04  1e-01  1e+02  5e-01  5e-01  0.9791  1e+00   1  1  1 | 20  0\n",
      " 4  +1.256e+00  -1.353e+04  +8e+04  9e-02  1e+02  4e-01  4e-01  0.4010  5e-01   1  2  1 |  7  4\n",
      " 5  +1.414e+00  -1.106e+04  +6e+04  7e-02  9e+01  3e-01  3e-01  0.9791  8e-01   1  2  2 | 12  0\n",
      " 6  +1.475e+00  -1.024e+04  +6e+04  6e-02  8e+01  3e-01  3e-01  0.9791  9e-01   1  1  2 | 16  0\n",
      " 7  +1.643e+00  -7.800e+03  +4e+04  4e-02  6e+01  2e-01  2e-01  0.4010  4e-01   2  2  2 |  6  4\n",
      " 8  +1.836e+00  -5.485e+03  +3e+04  3e-02  4e+01  2e-01  2e-01  0.5013  4e-01   2  1  1 |  6  3\n",
      " 9  +1.989e+00  -4.172e+03  +2e+04  2e-02  3e+01  1e-01  1e-01  0.4010  5e-01   2  1  2 |  6  4\n",
      "10  +2.167e+00  -2.933e+03  +2e+04  1e-02  2e+01  8e-02  9e-02  0.5013  5e-01   2  2  1 |  6  3\n",
      "11  +2.310e+00  -2.109e+03  +1e+04  1e-02  2e+01  6e-02  7e-02  0.4010  4e-01   2  1  1 |  5  4\n",
      "12  +2.451e+00  -1.267e+03  +9e+03  6e-03  1e+01  4e-02  4e-02  0.5013  3e-01   2  1  2 |  4  3\n",
      "13  +2.505e+00  -6.317e+02  +5e+03  3e-03  7e+00  2e-02  3e-02  0.5013  1e-01   2  2  1 |  2  3\n",
      "14  +2.420e+00  -2.095e+02  +2e+03  9e-04  3e+00  9e-03  1e-02  0.5988  3e-02   1  2  1 |  0  2\n",
      "15  +2.267e+00  -4.686e+01  +6e+02  2e-04  2e+00  3e-03  3e-03  0.7186  3e-02   1  1  1 |  0  1\n",
      "16  +2.112e+00  -1.021e+01  +2e+02  5e-05  2e+00  8e-04  8e-04  0.7430  3e-02   1  1  1 |  1  1\n",
      "17  +1.748e+00  -3.360e+00  +7e+01  2e-05  1e+00  3e-04  4e-04  0.6054  1e-01   1  1  1 |  2  2\n",
      "18  +1.198e+00  -5.703e-01  +3e+01  7e-06  8e-01  1e-04  2e-04  0.6266  8e-02   1  1  1 |  1  2\n",
      "19  +7.589e-01  -1.365e-01  +2e+01  4e-06  4e-01  7e-05  9e-05  0.5013  2e-01   1  1  1 |  2  3\n",
      "20  +4.693e-01  +1.097e-01  +8e+00  1e-06  2e-01  3e-05  4e-05  0.6266  1e-01   2  1  1 |  2  2\n",
      "21  +3.932e-01  +1.734e-01  +5e+00  9e-07  1e-01  2e-05  3e-05  0.5013  3e-01   1  1  1 |  4  3\n",
      "22  +3.624e-01  +1.987e-01  +4e+00  7e-07  8e-02  2e-06  2e-05  0.9791  7e-01   1  1  1 | 10  0\n",
      "23  +3.250e-01  +2.330e-01  +2e+00  4e-07  5e-02  1e-06  1e-05  0.5013  1e-01   1  1  1 |  3  3\n",
      "24  +3.049e-01  +2.492e-01  +1e+00  2e-07  3e-02  6e-07  6e-06  0.5013  2e-01   1  1  1 |  4  3\n",
      "25  +2.951e-01  +2.568e-01  +8e-01  2e-07  2e-02  3e-07  4e-06  0.6266  5e-01   2  1  1 |  7  2\n",
      "26  +2.940e-01  +2.575e-01  +8e-01  2e-07  2e-02  2e-07  4e-06  0.9791  9e-01   2  1  1 | 18  0\n",
      "27  +2.850e-01  +2.646e-01  +4e-01  8e-08  1e-02  9e-08  2e-06  0.5013  1e-01   2  1  1 |  3  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28  +2.818e-01  +2.670e-01  +3e-01  6e-08  8e-03  6e-08  2e-06  0.7833  7e-01   2  1  1 |  9  1\n",
      "29  +2.810e-01  +2.676e-01  +3e-01  6e-08  7e-03  6e-08  1e-06  0.9791  9e-01   2  1  1 | 15  0\n",
      "30  +2.780e-01  +2.699e-01  +2e-01  3e-08  4e-03  3e-08  9e-07  0.5013  2e-01   2  1  1 |  4  3\n",
      "31  +2.765e-01  +2.710e-01  +1e-01  2e-08  3e-03  2e-08  6e-07  0.7833  6e-01   2  1  1 |  8  1\n",
      "32  +2.759e-01  +2.714e-01  +1e-01  2e-08  2e-03  2e-08  5e-07  0.9791  8e-01   2  1  1 | 12  0\n",
      "33  +2.749e-01  +2.722e-01  +6e-02  1e-08  1e-03  1e-08  3e-07  0.5013  2e-01   1  1  1 |  4  3\n",
      "34  +2.742e-01  +2.726e-01  +3e-02  7e-09  9e-04  7e-09  2e-07  0.9791  6e-01   2  0  0 |  8  0\n",
      "35  +2.740e-01  +2.728e-01  +2e-02  5e-09  6e-04  5e-09  1e-07  0.9791  7e-01   1  1  1 | 10  0\n",
      "36  +2.737e-01  +2.730e-01  +2e-02  3e-09  4e-04  3e-09  8e-08  0.5013  3e-01   1  1  1 |  5  3\n",
      "37  +2.736e-01  +2.731e-01  +1e-02  2e-09  3e-04  2e-09  6e-08  0.9791  7e-01   2  0  0 | 10  0\n",
      "38  +2.735e-01  +2.732e-01  +7e-03  1e-09  2e-04  1e-09  4e-08  0.5013  2e-01   2  0  0 |  4  3\n",
      "39  +2.734e-01  +2.732e-01  +5e-03  9e-10  1e-04  9e-10  2e-08  0.9791  7e-01   2  0  0 |  9  0\n",
      "40  +2.734e-01  +2.733e-01  +3e-03  6e-10  8e-05  6e-10  2e-08  0.5013  3e-01   1  0  0 |  5  3\n",
      "41  +2.734e-01  +2.733e-01  +2e-03  4e-10  5e-05  4e-10  1e-08  0.9791  7e-01   2  0  0 | 10  0\n",
      "42  +2.733e-01  +2.733e-01  +1e-03  3e-10  3e-05  3e-10  7e-09  0.6266  4e-01   2  0  0 |  6  2\n",
      "43  +2.733e-01  +2.733e-01  +1e-03  3e-10  3e-05  3e-10  7e-09  0.9791  1e+00   2  0  0 | 20  0\n",
      "44  +2.733e-01  +2.733e-01  +9e-04  2e-10  2e-05  2e-10  4e-09  0.5013  3e-01   1  0  0 |  5  3\n",
      "45  +2.733e-01  +2.733e-01  +8e-04  1e-10  2e-05  1e-10  4e-09  0.9791  9e-01   2  0  0 | 14  0\n",
      "46  +2.733e-01  +2.733e-01  +6e-04  1e-10  2e-05  1e-10  3e-09  0.5013  6e-01   1  0  0 |  8  3\n",
      "47  +2.733e-01  +2.733e-01  +5e-04  1e-10  1e-05  1e-10  3e-09  0.9791  8e-01   1  0  0 | 13  0\n",
      "48  +2.733e-01  +2.733e-01  +4e-04  8e-11  1e-05  8e-11  2e-09  0.5013  6e-01   2  0  0 |  8  3\n",
      "49  +2.733e-01  +2.733e-01  +3e-04  7e-11  9e-06  7e-11  2e-09  0.9791  8e-01   1  0  0 | 13  0\n",
      "50  +2.733e-01  +2.733e-01  +3e-04  5e-11  7e-06  5e-11  1e-09  0.5013  6e-01   1  0  0 |  8  3\n",
      "51  +2.733e-01  +2.733e-01  +2e-04  5e-11  6e-06  5e-11  1e-09  0.9791  9e-01   1  0  0 | 14  0\n",
      "52  +2.733e-01  +2.733e-01  +2e-04  4e-11  5e-06  4e-11  9e-10  0.5013  6e-01   2  0  0 |  8  3\n",
      "53  +2.733e-01  +2.733e-01  +2e-04  3e-11  4e-06  3e-11  9e-10  0.9791  9e-01   1  0  1 | 18  0\n",
      "54  +2.733e-01  +2.733e-01  +1e-04  3e-11  4e-06  3e-11  7e-10  0.4010  5e-01   2  0  1 |  7  4\n",
      "55  +2.733e-01  +2.733e-01  +1e-04  2e-11  3e-06  2e-11  6e-10  0.9791  9e-01   2  0  0 | 15  0\n",
      "56  +2.733e-01  +2.733e-01  +1e-04  2e-11  3e-06  2e-11  5e-10  0.5013  7e-01   2  0  0 | 10  3\n",
      "57  +2.733e-01  +2.733e-01  +1e-04  2e-11  3e-06  2e-11  5e-10  0.9791  9e-01   1  0  0 | 16  0\n",
      "\n",
      "OPTIMAL (within feastol=2.6e-06, reltol=3.6e-04, abstol=9.9e-05).\n",
      "Runtime: 2338.843626 seconds.\n",
      "\n",
      "INPUT TRAIN:-1.0    24910\n",
      " 1.0     3424\n",
      "dtype: int64\n",
      "INPUT TEST:-1.0    24155\n",
      " 1.0     3378\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "x_control = x_control_train\n",
    "\n",
    "max_iters = 150\n",
    "max_iters_dccp = 75\n",
    "        \n",
    "num_points, num_features = x.shape\n",
    "w = cvxpy.Variable(num_features)\n",
    "\n",
    "np.random.seed(112233)\n",
    "w.value = np.random.rand(x.shape[1])\n",
    "\n",
    "constraints = []\n",
    "loss = cvxpy.sum(  cvxpy.logistic( cvxpy.multiply(-y, x*w) )  ) / num_points\n",
    "prob = cvxpy.Problem(cvxpy.Minimize(loss), constraints)\n",
    "\n",
    "tau =  float(0.005)\n",
    "mu = float(1.2)\n",
    "\n",
    "loss_function = \"logreg\" # perform the experiments with logistic regression\n",
    "EPS = float(1e-4)\n",
    "\n",
    "        \n",
    "prob.solve(method='dccp', tau=tau, mu=mu, tau_max=1e10, solver=cvxpy.ECOS, verbose=True, feastol=EPS, abstol=EPS, reltol=EPS, feastol_inacc=EPS, abstol_inacc=EPS, reltol_inacc=EPS,max_iters=max_iters, max_iter=max_iters_dccp)\n",
    "\n",
    "ret_w = np.array(w.value).flatten()\n",
    "sensitive_attrs = list(x_control_train.keys())\n",
    "\n",
    "print(\"INPUT TRAIN:\"+str(pd.value_counts(y_train)))\n",
    "print(\"INPUT TEST:\"+str(pd.value_counts(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "1.0    16913\n",
      "0.0    10620\n",
      "dtype: int64\n",
      "-1.0    24155\n",
      " 1.0     3378\n",
      "dtype: int64\n",
      "-0.020842      2\n",
      " 1.574523      2\n",
      " 2.255607      2\n",
      " 2.031716      2\n",
      " 0.746893      2\n",
      " 0.852894      2\n",
      " 1.477928      2\n",
      " 1.457478      2\n",
      " 1.585546      2\n",
      " 1.891058      2\n",
      " 2.245395      2\n",
      " 1.113750      2\n",
      " 1.566609      2\n",
      " 2.458973      1\n",
      " 0.667833      1\n",
      " 0.860191      1\n",
      " 2.274336      1\n",
      "-18.014385     1\n",
      "-0.462369      1\n",
      " 3.184204      1\n",
      "-2.566211      1\n",
      " 0.826014      1\n",
      " 3.181782      1\n",
      " 1.639158      1\n",
      " 1.401330      1\n",
      " 2.820239      1\n",
      " 0.956742      1\n",
      " 2.396763      1\n",
      " 1.697080      1\n",
      " 1.333718      1\n",
      "              ..\n",
      " 2.902354      1\n",
      " 2.780658      1\n",
      " 1.408145      1\n",
      " 1.055777      1\n",
      " 1.098304      1\n",
      " 54.307175     1\n",
      " 1.472571      1\n",
      " 5.431989      1\n",
      " 1.429634      1\n",
      " 2.009590      1\n",
      " 2.148188      1\n",
      "-14.046500     1\n",
      " 1.043092      1\n",
      " 4.610924      1\n",
      " 0.607925      1\n",
      " 120.917368    1\n",
      " 1.089740      1\n",
      " 1.289776      1\n",
      " 1.713013      1\n",
      " 1.237302      1\n",
      "-0.051147      1\n",
      "-9.415907      1\n",
      " 1.296210      1\n",
      " 3.484967      1\n",
      " 2.739026      1\n",
      " 0.724233      1\n",
      " 1.149569      1\n",
      " 1.935540      1\n",
      " 10.046637     1\n",
      "-0.793503      1\n",
      "Length: 27520, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#train_score, test_score, cov_all_train, cov_all_test, s_attr_to_fp_fn_train, s_attr_to_fp_fn_test = fdm.get_clf_stats(ret_w, x_train, y_train, x_control, x_test, y_test, x_control_test, list(sensitive_attrs))\n",
    "s_attr = sensitive_attrs[0]\n",
    "\n",
    "print(s_attr)\n",
    "print(pd.value_counts(x_control_test[s_attr]))\n",
    "distances_boundary_test = fdm.get_distance_boundary(ret_w, x_test, x_control_test[s_attr])\n",
    "print(pd.value_counts(y_test))\n",
    "print(pd.value_counts(distances_boundary_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec@500_abs: 0.29000\n",
      "doing:0.0\n",
      "-1.0    9326\n",
      " 1.0    1294\n",
      "dtype: int64\n",
      "------------------------------\n",
      "-1    10524\n",
      " 1       96\n",
      "dtype: int64\n",
      "------------------------------\n",
      "ALL POS=1294\n",
      "ALL NEG=9326\n",
      "doing:1\n",
      "-1.0    14829\n",
      " 1.0     2084\n",
      "dtype: int64\n",
      "------------------------------\n",
      "-1    16509\n",
      " 1      404\n",
      "dtype: int64\n",
      "------------------------------\n",
      "ALL POS=2084\n",
      "ALL NEG=14829\n",
      "recall non-white: 0.023957\n",
      "recall white: 0.054702\n",
      "recall ratio: 2.283388\n"
     ]
    }
   ],
   "source": [
    "k = 500\n",
    "all_class_labels_assigned_test = label_top_k(distances_boundary_test, k)\n",
    "prec_k = calc_prec(all_class_labels_assigned_test, y_test)\n",
    "print('prec@%s_abs: %.5f' % (k, prec_k))\n",
    "\n",
    "s_attr_to_fp_fn_test = fdm.get_fpr_fnr_sensitive_features(y_test, all_class_labels_assigned_test, x_control_test, sensitive_attrs, False)\n",
    "        \n",
    "    \n",
    "for s_attr in s_attr_to_fp_fn_test.keys():\n",
    "    for s_val in s_attr_to_fp_fn_test[s_attr].keys():\n",
    "        s_attr_to_fp_fn_test[s_attr][s_val]['recall'] = 1.000-s_attr_to_fp_fn_test[s_attr][s_val]['fnr']\n",
    "\n",
    "recall_nonwhite = s_attr_to_fp_fn_test['race'][0]['recall']\n",
    "recall_white = s_attr_to_fp_fn_test['race'][1]['recall']\n",
    "\n",
    "\n",
    "print('recall non-white: %.6f' % recall_nonwhite)\n",
    "print('recall white: %.6f' % recall_white)\n",
    "print('recall ratio: %.6f' % float(recall_white/recall_nonwhite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
